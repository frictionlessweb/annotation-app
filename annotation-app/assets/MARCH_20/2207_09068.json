{"title":"2207_09068","questions":{"topicTask":[{"id":0,"text":"Phrase-in-Context"},{"id":1,"text":"Phrase Similarity"},{"id":2,"text":"Phrase Retrieval"},{"id":3,"text":"Phrase-Sense Disambiguation"},{"id":4,"text":"Contextualized Phrase Embeddings"},{"id":5,"text":"Tradition"},{"id":6,"text":"Intangible Cultural Heritage"},{"id":7,"text":"Austria"},{"id":8,"text":"Massive Figure"},{"id":9,"text":"Word Sense Disambiguation"},{"id":10,"text":"Semantic Search"},{"id":11,"text":"Question Answering"},{"id":12,"text":"Semantic Representations"},{"id":13,"text":"Tokenized"}],"questionTask":[{"id":14,"text":"What is the purpose of the datasets PS, PR (including PR-pass and PR-page), and PSD that are being released to the public?"},{"id":19,"text":"What is the significance of the tradition of Samson in Lungau, Salzburg, and why was it entered into the UNESCO list of Intangible Cultural Heritage in Austria?"},{"id":24,"text":"What is the difference in performance between contextualized and non-contextualized embeddings in the PR-page example, as shown in Figure A4?"}],"statementsTask":[{"id":16,"text":"The purpose of releasing the datasets PS, PR (including PR-pass and PR-page), and PSD to the public is to provide human-annotated data for constructing three tasks, namely PS, PR, and PSD. These datasets are smaller in size compared to common QA datasets and contain shorter queries that are noun phrases instead of questions. The datasets are intended to be used for future research on PSD, which is a challenging task. State-of-the-art models perform relatively well on PR-pass and even PR-page but not on PSD, indicating the need for further research.\n\nSources:\n[0] We release three datasets PS, PR (including PR-pass and PR-page) and PSD to the public.\n[1] Table 2: Our PR-pass, PR-page and PSD datasets are smaller in size compared to common QA datasets and contain shorter queries that are noun phrases instead of questions.\n[2] Using the human-annotated data, we construct three tasks of PS, PR, and PSD (as summarized in Fig.","system":"gpt_retrieval"},{"id":18,"text":"The purpose of the datasets PS, PR (including PR-pass and PR-page), and PSD that are being released to the public is to provide a dataset of approximately 28K noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings [1]. The three tasks are: (1) PS, a binary classification task that asks whether two multi-word noun phrases are semantically similar or not given the same context sentence [2]; (2) PR, a task of finding in a given document a phrase that is semantically similar to the given query phrase [3]; and (3) PSD, a phrase-sense disambiguation task annotated by experts that requires understanding of phrase-senses in a passage [4]. These datasets are intended to evaluate the ability of models to leverage context to improve accuracy and to quantify the headroom for future research [5].\n\nSources:\n[1] To fill this gap, we propose PiC— a dataset of ∼28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings.\n[2] PS is a binary classification task that asks whether two mNPs are semantically similar or not given the same context sentence.\n[3] PR is a task of finding in a given document d a phrase p that is semantically similar to the given query phrase, which is the paraphrase q1 (the answer by annotators to Q1) or q2 (the answer to\n[4] Here, our PSD is the first phrase-sense disambiguation task annotated by experts and requires understanding of phrase-senses in a passage.\n[5] We test SotA models on PS, PR-pass, PR-page, and PSD to (1) assess how the models are able to leverage context to improve accuracy; and (2) quantify the headroom for future research."},{"id":21,"text":"The tradition of Samson is significant in Lungau, Salzburg as he is the emblem of the region and parades in his honor are held annually in ten villages of the Lungau and two villages in the north-west Styria (Austria). The tradition, which was first documented in 1635, involves carrying a massive figure made of wood or aluminum said to represent Samson. This tradition was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010. The significance of this tradition lies in its cultural heritage and historical value, which has been recognized by UNESCO. Therefore, the tradition of Samson in Lungau, Salzburg was entered into the UNESCO list of Intangible Cultural Heritage in Austria to preserve and promote its cultural significance.\n\nSources:\n[0] Samson is the emblem of Lungau, Salzburg and honor are held annually in ten villages of the Lungau villages in the north-west Styria (Austria).\n[1] Samson is the emblem of Lungau, Salzburg and parades in his honor are held annually in ten villages of the Lungau and two villages in the north-west Styria (Austria).\n[2] The tradition, which was first documented in 1635, was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010.","system":"gpt_retrieval"},{"id":23,"text":"The tradition of Samson in Lungau, Salzburg involves a young bachelor from the community carrying a massive figure made of wood or aluminum said to represent Samson during an annual parade held in ten villages of the Lungau and two villages in the north-west Styria (Austria) [1] [2] [3] [4] [5] [6] [7]. This tradition was first documented in 1635 and was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010 [8] [9]. The significance of this tradition is not mentioned in the sources.\n\nSources:\n[1] …………………………….….…………………………………………….page 1 …………………………….….………………………………………….. Samson is the emblem of Lungau, Salzburg and parades in his honor are held annually in ten villages of the Lungau and two villages in the north-west Styria (Austria).\n[2] During the parade, a young bachelor from the community carries a massive figure made of wood or aluminum said to represent Samson.\n[3] During the parade, a young bachelor from the S1 community carries a massive figure made of wood or aluminum said to represent Samson.\n[4] During the parade, a young bachelor from the S2 community carries a huge model made of wood\n[5] or aluminum said to represent Samson.\n[6] community carries a massive figure made of wood or aluminum said to represent Samson.\n[7] Samson is the emblem of Lungau, Salzburg and parades in his honor are held annually in ten villages of the Lungau and two villages in the north-west Styria (Austria).\n[8] The tradition, which was first documented in 1635, was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010………………………… …………………………………………………………………..............\n[9] The tradition, which was first documented in 1635, was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010."},{"id":26,"text":"The best top-1 accuracy scores on PR-pass for non-contextualized embeddings (USE-v5) and contextualized embeddings (BERT) are substantially lower than the non-expert baselines and Human Upperbound. However, after finetuning on PR-pass, the non-contextualized phrase embeddings improve substantially for most models at an average gain of +16.61 in top-1 accuracy. It is also mentioned that after training on PR-pass, the contextualized phrase embeddings perform much worse in the ranking experiments on PR-pass. Therefore, the non-contextualized embeddings perform better than the contextualized embeddings in the PR-page example. However, after finetuning on PR-pass, the non-contextualized embeddings improve substantially.\n\nSources:\n[0] Interestingly, after training on PR-pass, their contextualized phrase embeddings perform much worse in the ranking experiments on PR-pass (Appendix (<>)H).\n[1] Second, the best top-1 accuracy scores on PR-pass for non-contextualized (USE-v5; 43.36%) and contextualized (BERT; 47.44%) embeddings are substantially lower than the non-expert baselines (73.60%; Table (<>)4) and Human Upperbound (95%).","system":"gpt_retrieval"},{"id":28,"text":"The best top-1 accuracy scores on PR-pass for non-contextualized (USE-v5; 43.36%) and contextualized (BERT; 47.44%) embeddings are substantially lower than the non-expert baselines (73.60%) and Human Upperbound (95%) [1]. However, on PR-page, where the documents are substantially longer than the documents of PR-pass, the accuracy of SS models only drops slightly (from ∼94% to ∼85–89% EM) [2]. Table A3 shows that the ranking accuracy of contextualized phrase embeddings drops significantly, -11.95 points on average, compared to before finetuning on PR-pass [3]. Figure A11 shows that a ranking model based on the non-contextualized embeddings of USE-v5 fails to retrieve the correct answer “continued risk” for the query “sustained threat” in the PR-page example [4]. Therefore, the contextualized embeddings perform worse than non-contextualized embeddings in the PR-page example.\n\nSources:\n[1] Second, the best top-1 accuracy scores on PR-pass for non-contextualized (USE-v5; 43.36%) and contextualized (BERT; 47.44%) embeddings are substantially lower than the non-expert baselines (73.60%; Table (<>)4) and Human Upperbound (95%).\n[2] Surprisingly, on PR-page where the documents are substantially longer (around 12×) than the documents of PR-pass, SS models’ accuracy only drops slightly (from ∼94% to ∼85–89% EM; Table (<>)6b).\n[3] Table A3: Ranking accuracy (%) on PR-page using the state-of-the-art pretrained phrase embeddings (a) and those finetuned on PR-pass via SS-style training (b).\n[4] Figure A11: A ranking model based on the non-contextualized embeddings of USE-v5 fails to retrieve the correct answer “continued risk” for the query “sustained threat” in the PR-page example (which contains a document d and a query q)."}],"qaTask":[{"id":15,"question":"What is the purpose of the datasets PS, PR (including PR-pass and PR-page), and PSD that are being released to the public?","answer":"The purpose of releasing the datasets PS, PR (including PR-pass and PR-page), and PSD to the public is to provide human-annotated data for constructing three tasks, namely PS, PR, and PSD. These datasets are smaller in size compared to common QA datasets and contain shorter queries that are noun phrases instead of questions. The datasets are intended to be used for future research on PSD, which is a challenging task. State-of-the-art models perform relatively well on PR-pass and even PR-page but not on PSD, indicating the need for further research.\n\nSources:\n[0] We release three datasets PS, PR (including PR-pass and PR-page) and PSD to the public.\n[1] Table 2: Our PR-pass, PR-page and PSD datasets are smaller in size compared to common QA datasets and contain shorter queries that are noun phrases instead of questions.\n[2] Using the human-annotated data, we construct three tasks of PS, PR, and PSD (as summarized in Fig.","system":"gpt_retrieval"},{"id":17,"question":"What is the purpose of the datasets PS, PR (including PR-pass and PR-page), and PSD that are being released to the public?","answer":"The purpose of the datasets PS, PR (including PR-pass and PR-page), and PSD that are being released to the public is to provide a dataset of approximately 28K noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings [1]. The three tasks are: (1) PS, a binary classification task that asks whether two multi-word noun phrases are semantically similar or not given the same context sentence [2]; (2) PR, a task of finding in a given document a phrase that is semantically similar to the given query phrase [3]; and (3) PSD, a phrase-sense disambiguation task annotated by experts that requires understanding of phrase-senses in a passage [4]. These datasets are intended to evaluate the ability of models to leverage context to improve accuracy and to quantify the headroom for future research [5].\n\nSources:\n[1] To fill this gap, we propose PiC— a dataset of ∼28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings.\n[2] PS is a binary classification task that asks whether two mNPs are semantically similar or not given the same context sentence.\n[3] PR is a task of finding in a given document d a phrase p that is semantically similar to the given query phrase, which is the paraphrase q1 (the answer by annotators to Q1) or q2 (the answer to\n[4] Here, our PSD is the first phrase-sense disambiguation task annotated by experts and requires understanding of phrase-senses in a passage.\n[5] We test SotA models on PS, PR-pass, PR-page, and PSD to (1) assess how the models are able to leverage context to improve accuracy; and (2) quantify the headroom for future research."},{"id":20,"question":"What is the significance of the tradition of Samson in Lungau, Salzburg, and why was it entered into the UNESCO list of Intangible Cultural Heritage in Austria?","answer":"The tradition of Samson is significant in Lungau, Salzburg as he is the emblem of the region and parades in his honor are held annually in ten villages of the Lungau and two villages in the north-west Styria (Austria). The tradition, which was first documented in 1635, involves carrying a massive figure made of wood or aluminum said to represent Samson. This tradition was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010. The significance of this tradition lies in its cultural heritage and historical value, which has been recognized by UNESCO. Therefore, the tradition of Samson in Lungau, Salzburg was entered into the UNESCO list of Intangible Cultural Heritage in Austria to preserve and promote its cultural significance.\n\nSources:\n[0] Samson is the emblem of Lungau, Salzburg and honor are held annually in ten villages of the Lungau villages in the north-west Styria (Austria).\n[1] Samson is the emblem of Lungau, Salzburg and parades in his honor are held annually in ten villages of the Lungau and two villages in the north-west Styria (Austria).\n[2] The tradition, which was first documented in 1635, was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010.","system":"gpt_retrieval"},{"id":22,"question":"What is the significance of the tradition of Samson in Lungau, Salzburg, and why was it entered into the UNESCO list of Intangible Cultural Heritage in Austria?","answer":"The tradition of Samson in Lungau, Salzburg involves a young bachelor from the community carrying a massive figure made of wood or aluminum said to represent Samson during an annual parade held in ten villages of the Lungau and two villages in the north-west Styria (Austria) [1] [2] [3] [4] [5] [6] [7]. This tradition was first documented in 1635 and was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010 [8] [9]. The significance of this tradition is not mentioned in the sources.\n\nSources:\n[1] …………………………….….…………………………………………….page 1 …………………………….….………………………………………….. Samson is the emblem of Lungau, Salzburg and parades in his honor are held annually in ten villages of the Lungau and two villages in the north-west Styria (Austria).\n[2] During the parade, a young bachelor from the community carries a massive figure made of wood or aluminum said to represent Samson.\n[3] During the parade, a young bachelor from the S1 community carries a massive figure made of wood or aluminum said to represent Samson.\n[4] During the parade, a young bachelor from the S2 community carries a huge model made of wood\n[5] or aluminum said to represent Samson.\n[6] community carries a massive figure made of wood or aluminum said to represent Samson.\n[7] Samson is the emblem of Lungau, Salzburg and parades in his honor are held annually in ten villages of the Lungau and two villages in the north-west Styria (Austria).\n[8] The tradition, which was first documented in 1635, was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010………………………… …………………………………………………………………..............\n[9] The tradition, which was first documented in 1635, was entered into the UNESCO list of Intangible Cultural Heritage in Austria in 2010."},{"id":25,"question":"What is the difference in performance between contextualized and non-contextualized embeddings in the PR-page example, as shown in Figure A4?","answer":"The best top-1 accuracy scores on PR-pass for non-contextualized embeddings (USE-v5) and contextualized embeddings (BERT) are substantially lower than the non-expert baselines and Human Upperbound. However, after finetuning on PR-pass, the non-contextualized phrase embeddings improve substantially for most models at an average gain of +16.61 in top-1 accuracy. It is also mentioned that after training on PR-pass, the contextualized phrase embeddings perform much worse in the ranking experiments on PR-pass. Therefore, the non-contextualized embeddings perform better than the contextualized embeddings in the PR-page example. However, after finetuning on PR-pass, the non-contextualized embeddings improve substantially.\n\nSources:\n[0] Interestingly, after training on PR-pass, their contextualized phrase embeddings perform much worse in the ranking experiments on PR-pass (Appendix (<>)H).\n[1] Second, the best top-1 accuracy scores on PR-pass for non-contextualized (USE-v5; 43.36%) and contextualized (BERT; 47.44%) embeddings are substantially lower than the non-expert baselines (73.60%; Table (<>)4) and Human Upperbound (95%).","system":"gpt_retrieval"},{"id":27,"question":"What is the difference in performance between contextualized and non-contextualized embeddings in the PR-page example, as shown in Figure A4?","answer":"The best top-1 accuracy scores on PR-pass for non-contextualized (USE-v5; 43.36%) and contextualized (BERT; 47.44%) embeddings are substantially lower than the non-expert baselines (73.60%) and Human Upperbound (95%) [1]. However, on PR-page, where the documents are substantially longer than the documents of PR-pass, the accuracy of SS models only drops slightly (from ∼94% to ∼85–89% EM) [2]. Table A3 shows that the ranking accuracy of contextualized phrase embeddings drops significantly, -11.95 points on average, compared to before finetuning on PR-pass [3]. Figure A11 shows that a ranking model based on the non-contextualized embeddings of USE-v5 fails to retrieve the correct answer “continued risk” for the query “sustained threat” in the PR-page example [4]. Therefore, the contextualized embeddings perform worse than non-contextualized embeddings in the PR-page example.\n\nSources:\n[1] Second, the best top-1 accuracy scores on PR-pass for non-contextualized (USE-v5; 43.36%) and contextualized (BERT; 47.44%) embeddings are substantially lower than the non-expert baselines (73.60%; Table (<>)4) and Human Upperbound (95%).\n[2] Surprisingly, on PR-page where the documents are substantially longer (around 12×) than the documents of PR-pass, SS models’ accuracy only drops slightly (from ∼94% to ∼85–89% EM; Table (<>)6b).\n[3] Table A3: Ranking accuracy (%) on PR-page using the state-of-the-art pretrained phrase embeddings (a) and those finetuned on PR-pass via SS-style training (b).\n[4] Figure A11: A ranking model based on the non-contextualized embeddings of USE-v5 fails to retrieve the correct answer “continued risk” for the query “sustained threat” in the PR-page example (which contains a document d and a query q)."},{"id":29,"question":"What is this paper about?","answer":"This paper is about the process of inventing and refining the data collection process in order to arrive at the final list of steps. The paper describes the data construction process, annotation and verification methods. It also includes a figure that shows two associated Wikipedia pages and expert annotations given a phrase. The paper mentions a Wikipedia page that is truncated to fit into a single manuscript page. Additionally, there are references to articles and PR-page examples.\n\nSources:\n[0] process of inventing and refining the data collection process in order to arrive at the final list of steps as presented in this paper.\n[1] We describe the data construction process, annotation and verification methods in our paper (See Sec.\n[2] Figure 1: Given a phrase, two associated Wikipedia pages, and expert annotations, i.e.\n[3] We have added this note in the Data Collection description.\n[4] articles\n[5] (d) A PR-page example."},{"id":30,"question":"What is this paper about?","answer":"This paper proposes a dataset called PiC, which contains approximately 28,000 noun phrases accompanied by their contextual Wikipedia pages, and a suite of three tasks for training and evaluating phrase embeddings [1]. The paper discusses the challenges of learning contextualized phrase embeddings and the lack of a human-annotated benchmark for testing machine understanding of phrase semantics given a context sentence or paragraph [2]. The dataset is rigorously annotated and verified by two groups of annotators: linguistics experts on Upwork.com and non-experts on Amazon Mechanical Turk [3]. The paper also describes the three tasks in the PiC suite, which include phrase similarity, phrase ranking, and span selection [4]. The authors found that training on PiC improves ranking-models' accuracy and remarkably pushes span-selection models near human-accuracy [5]. The paper also discusses the challenges of incorporating context effectively into phrase embeddings and the need for future work to learn more robust phrase embeddings for ranking [6]. The authors propose a hybrid approach to evaluate dataset quality, leveraging both linguistic experts and non-experts, and ensuring high quality for 30,042 queries and 15,021 Yes/No answers at lower cost compared to inter-annotator agreement via two rounds [7]. The paper concludes by releasing three datasets PS, PR (including PR-pass and PR-page), and PSD to the public under the CC-BY-NC 4.0 license [8].\n\nSources:\n[1] To fill this gap, we propose PiC— a dataset of ∼28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings.\n[2] While contextualized word embeddings have been a de-facto standard, learning contextualized phrase embeddings is less explored and being hindered by the lack of a human-annotated benchmark that tests machine understanding of phrase semantics given a context sentence or paragraph (instead of phrases alone).\n[3] Our ∼28K-example dataset is rigorously (a) annotated and verified by two groups of annotators: linguistics experts on (<Upwork.com>)Upwork.com and non-experts on Amazon Mechanical Turk (MTurk); and then (b) tested by models, linguists, and graduate students.\n[4] To advance the development of contextualized phrase embeddings, we propose Phrase-in-Context (PiC), a suite of three tasks: (1) Phrase Similarity (PS), i.e.\n[5] Training on PiC improves ranking-models’ accuracy and remarkably pushes span-selection (SS) models (i.e., predicting the start and end index of the target phrase) near human-accuracy, which is 95% Exact Match (EM) on semantic search given a query phrase and a passage.\n[6] Aligned with (<>)Yu and Ettinger ((<>)2020), we find that incorporating context effectively into phrase embeddings is an open research challenge.\n[7] There are two common methods for evaluation of dataset quality: (1) Verify only a small, random subset ((<>)Pilehvar and Camacho-Collados, (<>)2019) to estimate the quality of the full dataset or (2) verifying the entire dataset with multiple annotators and use the inter-annotator agreement (IAA) to control quality ((<>)Bowman et al., (<>)2015; (<>)Kwiatkowski et al., (<>)2019).\n[8] We release three datasets PS, PR (including PR-pass and PR-page) and PSD to the public."},{"id":31,"question":"How many examples are being annotated?","answer":"It is not explicitly stated how many examples are being annotated in the given sources. However, source 3 mentions that 19,500 examples are selected for annotation after removing proper nouns and phrases with missing information. Additionally, source 6 mentions positive examples being constructed from annotated data. Therefore, it can be inferred that at least 19,500 examples are being annotated.\n\nSources:\n[0] Remove proper nouns and phrases with missing information and select top 19,500 examples for annotation."},{"id":32,"question":"How many examples are being annotated?","answer":"The dataset consists of approximately 28,000 noun phrases accompanied by their contextual Wikipedia pages [1]. Via Upwork, 13 linguistics experts were hired to annotate 15,021 out of 19,500 examples [2]. After two verification rounds, the final dataset consists of 28,325 paraphrases and 13,413 Y/N labels [3]. Therefore, 15,021 examples were annotated by linguistics experts.\n\nSources:\n[1] To fill this gap, we propose PiC— a dataset of ∼28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings.\n[2] Via Upwork, we hire 13 linguistics experts who are native English speakers at a rate of $30/hour to annotate 15,021 out of 19,500 examples.\n[3] After these two verification rounds, we discard all the examples where Up-work verifiers reject and arrive at the final 28,325 paraphrases and 13,413 Y/N labels (i.e."},{"id":33,"question":"What is the annotation cost?","answer":"The annotation cost is $30/hour for 13 linguistics experts to annotate 15,021 out of 19,500 examples. The estimated yield of annotated triplets is ∼15K after the human annotation process where annotators are allowed to skip the cases they are not confident labeling. The most polysemous 19,500 mNPs are then passed to experts for annotation. There is no specific mention of the total cost of annotation.\n\nSources:\n[0] annotated\n[1] Via Upwork, we hire 13 linguistics experts who are native English speakers at a rate of $30/hour to annotate 15,021 out of 19,500 examples.\n[2] That is, 19,500 triplets are estimated to yield ∼15K annotated triplets (the target size based on our budget) after the human annotation process where annotators are allowed to skip the cases they are not confident labeling.\n[3] The most polysemous 19,500 mNPs are then passed to experts for annotation (Sec."},{"id":34,"question":"What is the annotation cost?","answer":"The total fee for both MTurk and Upwork combined is around USD 30,000 [1]. The verification process costs $5.6 per 30 verified answers in around ∼20 minutes, including AMT fees, and 1 token to Gorilla to a single MTurk verifier [2]. The proposed hybrid approach aims to ensure high quality for 30,042 queries and 15,021 Yes/No answers at a lower cost compared to IAA via two rounds [3]. The cost of annotation is not explicitly mentioned, but the approach is designed to be cost-effective.\n\nSources:\n[1] The total fee for both MTurk and Upwork combined is around USD 30,000.\n[2] Per 30 verified answers in around ∼20 minutes, the verification process costs us $5.6 (AMT fees included) and 1 token to Gorilla to a single MTurk verifier.\n[3] We propose a hybrid approach to evaluate (leveraging both linguistic experts and non-experts) and ensure high quality for 30,042 queries and 15,021 Yes/No answers at lower cost compared to IAA via two rounds:"}]}}