{"title":"toolformer","questions":{"topicTask":[{"id":0,"text":"Language Models"},{"id":1,"text":"Toolformer"},{"id":2,"text":"API Calls"},{"id":3,"text":"Zero-shot Performance"},{"id":4,"text":"GPT-3"}],"questionTask":[{"id":5,"text":"What is the purpose of the Toolformer model and how does it make use of API calls?"},{"id":10,"text":"What downstream tasks were the models evaluated on and how did Toolformer perform compared to other models?"},{"id":15,"text":"How does the addition of calls to a Question Answering API help in completing the text and what are some examples of API calls that can be made?"}],"statementsTask":[{"id":7,"text":"The purpose of the Toolformer model is to learn how to use different tools such as search engines, calculators, and translation systems via simple API calls. It is trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. Toolformer makes use of API calls to ensure that they happen close to where the information provided by the API is actually helpful for the model. Using API calls consistently improves Toolformer’s performance for all languages, suggesting that it has learned to make use of the machine translation tool. The model autonomously decides to call different APIs to obtain information that is useful for completing a piece of text. Toolformer's self-supervised nature enables it to learn how and when to use tools without requiring a specific prompt that shows task-specific examples of how a tool could be used. Additionally, Toolformer learns for itself to explicitly ask for the right information. The model is designed to ensure that language modeling performance of Toolformer does not degrade through finetuning with API calls. Models trained with Toolformer are often sensitive to the exact wording of their input when deciding whether or not to call an API.\n\nSources:\n[0] We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.\n[1] We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls.\n[2] to make sure that API calls happen close to where the information provided by the API is actually helpful for the model.\n[3] Using API calls consistently improves Toolformer’s performance for all languages, suggesting that it has learned to make use of the machine translation tool.\n[4] Beyond this, we found models trained with Toolformer to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero-and few-shot settings ((<>)Jiang et al., (<>)2020; (<>)Schick (<>)and Schütze, (<>)2021a).\n[5] The model autonomously decides to call different APIs (from top to bottom: a question answering system, a calculator, a machine translation system, and a Wikipedia search engine) to obtain information that is useful for completing a piece of text.\n[6] In contrast, the self-supervised nature of Toolformer enables it to learn how and when to use tools without requiring a specific prompt that shows task-specific examples of how a tool could be used.","system":"gpt_retrieval"},{"id":9,"text":"The purpose of the Toolformer model is to enable language models to use external tools such as search engines, calculators, and translation systems via simple API calls, while still maintaining their core language modeling abilities [1] [2] [3]. Toolformer is trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction [4]. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API [5]. Toolformer incorporates a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar [6]. The model is trained to decide when and how to use which tool, based purely on its own feedback [7]. The model learns to use tools by finetuning on a large number of sampled API calls that are filtered based on whether they are helpful to the model in predicting future tokens [8]. The model independently decides to ask the question answering tool for the required information in almost all cases, and for only very few examples, it uses a different tool or no tool at all [9]. The model learns how to make good use of API calls as it scales up in size [10].\n\nSources:\n[1] Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale.\n[2] In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.\n[3] We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.\n[4] We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.\n[5] This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API.\n[6] We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar.\n[7] As a consequence, finetuning M on C ∗ exposes it to the same content as finetuning on C. Moreover, as API calls are inserted in exactly those positions and with exactly those inputs that help M predict future tokens, finetuning on C∗ enables the language model to decide when and how to use which tool, based purely on its own feedback.\n[8] This is done by finetuning on a large number of sampled API calls that are filtered based on whether they\n[9] This is achieved because the model independently decides to ask the question answering tool for the required information in almost all cases (98.1%); for only very few examples, it uses a different tool (0.7%) or no tool at all (1.2%).\n[10] While API calls are not helpful to the smallest models, larger models learn how to make good use of them."},{"id":12,"text":"Toolformer was evaluated on various downstream tasks and achieved substantially improved zero-shot performance, often competitive with much larger models, without sacrificing its core language modeling abilities. Toolformer clearly outperformed all other models based on GPT-J, relying mostly on the Wikipedia search API (99.3%) to find relevant information. Toolformer clearly outperformed the baseline models by 11.7, 5.2, and 18.6 points, respectively. The specific downstream tasks that were evaluated are not mentioned in the sources provided.\n\nSources:\n[0] Once again, Toolformer clearly outperforms all other models based on GPT-J, this time mostly relying on the Wikipedia search API (99.3%) to find relevant information.\n[1] Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n[2] We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls.","system":"gpt_retrieval"},{"id":14,"text":"The models were evaluated on a variety of downstream tasks, including SQuAD, Google-RE, T-REx, ASDiv, SVAMP, MAWPS, Web Questions, Natural Questions, TriviaQA, and MLQA [1] [2] [3] [4] [5]. Toolformer, which is based on a pretrained GPT-J model with 6.7B parameters, achieved much stronger zero-shot results compared to a much larger GPT-3 model and other baseline models for most tasks [6] [7] [8] [9] [10] [11] [12] [13] [14]. Additionally, allowing the model to make API calls more than doubled performance for all tasks and consistently improved Toolformer's performance for all languages [15] [16].\n\nSources:\n[1] We evaluate all models on a variety of downstream tasks.\n[2] We evaluate our models on the SQuAD, Google-RE and T-REx subsets of the LAMA benchmark ((<>)Petroni et al., (<>)2019).\n[3] We test mathematical reasoning abilities on ASDiv ((<>)Miao et al., (<>)2020), SVAMP ((<>)Patel et al., (<>)2021) and the MAWPS benchmark ((<>)Koncel-Kedziorski et al., (<>)2016).\n[4] We look at Web Questions ((<>)Berant et al., (<>)2013), Natural Questions ((<>)Kwiatkowski et al., (<>)2019) and TriviaQA ((<>)Joshi et al., (<>)2017), the three question answering datasets considered by (<>)Brown et al.\n[5] We evaluate Toolformer and all baseline models on MLQA ((<>)Lewis et al., (<>)2019), a multilingual question-answering benchmark.\n[6] We conduct experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model ((<>)Wang and Komat-(<>)suzaki, (<>)2021) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model ((<>)Brown et al., (<>)2020) and\n[7] Nonetheless, allowing the model to make API calls more than doubles performance for all tasks, and also clearly outperforms the much larger OPT and GPT-3 models.\n[8] Crucially, Toolformer clearly outperforms these baseline models, improving upon the best baseline by 11.7, 5.2 and 18.6 points, respectively.\n[9] Toolformer uses the question answering tool for most examples, clearly outperforming all baselines of the same size and achieving results competitive with GPT-3 (175B).\n[10] Toolformer makes use of the calculator tool for most examples, clearly outperforming even OPT (66B) and GPT-3 (175B).\n[11] Table (<>)4 shows results for all benchmarks.\n[12] Using the Wikipedia search tool for most examples, Toolformer clearly outperforms baselines of the same size, but falls short of GPT-3 (175B).\n[13] Once again, Toolformer clearly outperforms all other models based on GPT-J, this time mostly relying on the Wikipedia search API (99.3%) to find relevant information.\n[14] Results shown in Table (<>)7 illustrate that Tool-former outperforms all baselines for both TEMPLAMA and DATESET.\n[15] Nonetheless, allowing the model to make API calls more than doubles performance for all tasks, and also clearly outperforms the much larger OPT and GPT-3 models.\n[16] Using API calls consistently improves Toolformer’s performance for all languages, suggesting that it has learned to make use of the machine translation tool."},{"id":17,"text":"The addition of calls to a Question Answering API helps in completing the text by providing information required to complete it. The model autonomously decides to call different APIs, such as a question answering system, a calculator, a machine translation system, and a Wikipedia search engine, to obtain useful information. The approach involves sampling a position and corresponding API call candidates to make sure that API calls happen close to where the information provided by the API is actually helpful for the model. Adding API calls does not come at a cost in terms of perplexity for language modeling without any API calls. Some examples of API calls that can be made include those provided in sources 3, 4, and 5.\n\nSources:\n[0] Your task is to add calls to a Question Answering API to a piece of text.\n[1] The model autonomously decides to call different APIs (from top to bottom: a question answering system, a calculator, a machine translation system, and a Wikipedia search engine) to obtain information that is useful for completing a piece of text.\n[2] Here are some examples of API calls:\n[3] Here are some examples of API calls:\n[4] Here are some examples of API calls:\n[5] Figure 2: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we first sample a position i and corresponding12 API call candidates ci , ci , .\n[6] to make sure that API calls happen close to where the information provided by the API is actually helpful for the model.\n[7] Adding API calls comes without a cost in terms of perplexity for language modeling without any API calls.","system":"gpt_retrieval"},{"id":19,"text":"The addition of calls to a Question Answering API helps in completing the text by providing relevant information to fill in the missing parts of the text [1]. For example, given the input text \"Joe Biden was born in Scranton, Pennsylvania,\" the output text could be \"Joe Biden was born in [QA(\"Where was Joe Biden born?\")],\" where the API call to the Question Answering tool provides the missing information about Joe Biden's birthplace [2]. Other examples of API calls that can be made include \"What other name is Coca-Cola known by?\" and \"Who manufactures Coca-Cola?\" [3].\n\nSources:\n[1] We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.\n[2] Input: Joe Biden was born in Scranton, Pennsylvania.\n[3] Output: Coca-Cola, or [QA(\"What other name is Coca-Cola known by?\")]"}],"qaTask":[{"id":6,"question":"What is the purpose of the Toolformer model and how does it make use of API calls?","answer":"The purpose of the Toolformer model is to learn how to use different tools such as search engines, calculators, and translation systems via simple API calls. It is trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. Toolformer makes use of API calls to ensure that they happen close to where the information provided by the API is actually helpful for the model. Using API calls consistently improves Toolformer’s performance for all languages, suggesting that it has learned to make use of the machine translation tool. The model autonomously decides to call different APIs to obtain information that is useful for completing a piece of text. Toolformer's self-supervised nature enables it to learn how and when to use tools without requiring a specific prompt that shows task-specific examples of how a tool could be used. Additionally, Toolformer learns for itself to explicitly ask for the right information. The model is designed to ensure that language modeling performance of Toolformer does not degrade through finetuning with API calls. Models trained with Toolformer are often sensitive to the exact wording of their input when deciding whether or not to call an API.\n\nSources:\n[0] We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.\n[1] We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls.\n[2] to make sure that API calls happen close to where the information provided by the API is actually helpful for the model.\n[3] Using API calls consistently improves Toolformer’s performance for all languages, suggesting that it has learned to make use of the machine translation tool.\n[4] Beyond this, we found models trained with Toolformer to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero-and few-shot settings ((<>)Jiang et al., (<>)2020; (<>)Schick (<>)and Schütze, (<>)2021a).\n[5] The model autonomously decides to call different APIs (from top to bottom: a question answering system, a calculator, a machine translation system, and a Wikipedia search engine) to obtain information that is useful for completing a piece of text.\n[6] In contrast, the self-supervised nature of Toolformer enables it to learn how and when to use tools without requiring a specific prompt that shows task-specific examples of how a tool could be used.","system":"gpt_retrieval"},{"id":8,"question":"What is the purpose of the Toolformer model and how does it make use of API calls?","answer":"The purpose of the Toolformer model is to enable language models to use external tools such as search engines, calculators, and translation systems via simple API calls, while still maintaining their core language modeling abilities [1] [2] [3]. Toolformer is trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction [4]. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API [5]. Toolformer incorporates a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar [6]. The model is trained to decide when and how to use which tool, based purely on its own feedback [7]. The model learns to use tools by finetuning on a large number of sampled API calls that are filtered based on whether they are helpful to the model in predicting future tokens [8]. The model independently decides to ask the question answering tool for the required information in almost all cases, and for only very few examples, it uses a different tool or no tool at all [9]. The model learns how to make good use of API calls as it scales up in size [10].\n\nSources:\n[1] Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale.\n[2] In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.\n[3] We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.\n[4] We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.\n[5] This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API.\n[6] We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar.\n[7] As a consequence, finetuning M on C ∗ exposes it to the same content as finetuning on C. Moreover, as API calls are inserted in exactly those positions and with exactly those inputs that help M predict future tokens, finetuning on C∗ enables the language model to decide when and how to use which tool, based purely on its own feedback.\n[8] This is done by finetuning on a large number of sampled API calls that are filtered based on whether they\n[9] This is achieved because the model independently decides to ask the question answering tool for the required information in almost all cases (98.1%); for only very few examples, it uses a different tool (0.7%) or no tool at all (1.2%).\n[10] While API calls are not helpful to the smallest models, larger models learn how to make good use of them."},{"id":11,"question":"What downstream tasks were the models evaluated on and how did Toolformer perform compared to other models?","answer":"Toolformer was evaluated on various downstream tasks and achieved substantially improved zero-shot performance, often competitive with much larger models, without sacrificing its core language modeling abilities. Toolformer clearly outperformed all other models based on GPT-J, relying mostly on the Wikipedia search API (99.3%) to find relevant information. Toolformer clearly outperformed the baseline models by 11.7, 5.2, and 18.6 points, respectively. The specific downstream tasks that were evaluated are not mentioned in the sources provided.\n\nSources:\n[0] Once again, Toolformer clearly outperforms all other models based on GPT-J, this time mostly relying on the Wikipedia search API (99.3%) to find relevant information.\n[1] Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n[2] We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls.","system":"gpt_retrieval"},{"id":13,"question":"What downstream tasks were the models evaluated on and how did Toolformer perform compared to other models?","answer":"The models were evaluated on a variety of downstream tasks, including SQuAD, Google-RE, T-REx, ASDiv, SVAMP, MAWPS, Web Questions, Natural Questions, TriviaQA, and MLQA [1] [2] [3] [4] [5]. Toolformer, which is based on a pretrained GPT-J model with 6.7B parameters, achieved much stronger zero-shot results compared to a much larger GPT-3 model and other baseline models for most tasks [6] [7] [8] [9] [10] [11] [12] [13] [14]. Additionally, allowing the model to make API calls more than doubled performance for all tasks and consistently improved Toolformer's performance for all languages [15] [16].\n\nSources:\n[1] We evaluate all models on a variety of downstream tasks.\n[2] We evaluate our models on the SQuAD, Google-RE and T-REx subsets of the LAMA benchmark ((<>)Petroni et al., (<>)2019).\n[3] We test mathematical reasoning abilities on ASDiv ((<>)Miao et al., (<>)2020), SVAMP ((<>)Patel et al., (<>)2021) and the MAWPS benchmark ((<>)Koncel-Kedziorski et al., (<>)2016).\n[4] We look at Web Questions ((<>)Berant et al., (<>)2013), Natural Questions ((<>)Kwiatkowski et al., (<>)2019) and TriviaQA ((<>)Joshi et al., (<>)2017), the three question answering datasets considered by (<>)Brown et al.\n[5] We evaluate Toolformer and all baseline models on MLQA ((<>)Lewis et al., (<>)2019), a multilingual question-answering benchmark.\n[6] We conduct experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model ((<>)Wang and Komat-(<>)suzaki, (<>)2021) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model ((<>)Brown et al., (<>)2020) and\n[7] Nonetheless, allowing the model to make API calls more than doubles performance for all tasks, and also clearly outperforms the much larger OPT and GPT-3 models.\n[8] Crucially, Toolformer clearly outperforms these baseline models, improving upon the best baseline by 11.7, 5.2 and 18.6 points, respectively.\n[9] Toolformer uses the question answering tool for most examples, clearly outperforming all baselines of the same size and achieving results competitive with GPT-3 (175B).\n[10] Toolformer makes use of the calculator tool for most examples, clearly outperforming even OPT (66B) and GPT-3 (175B).\n[11] Table (<>)4 shows results for all benchmarks.\n[12] Using the Wikipedia search tool for most examples, Toolformer clearly outperforms baselines of the same size, but falls short of GPT-3 (175B).\n[13] Once again, Toolformer clearly outperforms all other models based on GPT-J, this time mostly relying on the Wikipedia search API (99.3%) to find relevant information.\n[14] Results shown in Table (<>)7 illustrate that Tool-former outperforms all baselines for both TEMPLAMA and DATESET.\n[15] Nonetheless, allowing the model to make API calls more than doubles performance for all tasks, and also clearly outperforms the much larger OPT and GPT-3 models.\n[16] Using API calls consistently improves Toolformer’s performance for all languages, suggesting that it has learned to make use of the machine translation tool."},{"id":16,"question":"How does the addition of calls to a Question Answering API help in completing the text and what are some examples of API calls that can be made?","answer":"The addition of calls to a Question Answering API helps in completing the text by providing information required to complete it. The model autonomously decides to call different APIs, such as a question answering system, a calculator, a machine translation system, and a Wikipedia search engine, to obtain useful information. The approach involves sampling a position and corresponding API call candidates to make sure that API calls happen close to where the information provided by the API is actually helpful for the model. Adding API calls does not come at a cost in terms of perplexity for language modeling without any API calls. Some examples of API calls that can be made include those provided in sources 3, 4, and 5.\n\nSources:\n[0] Your task is to add calls to a Question Answering API to a piece of text.\n[1] The model autonomously decides to call different APIs (from top to bottom: a question answering system, a calculator, a machine translation system, and a Wikipedia search engine) to obtain information that is useful for completing a piece of text.\n[2] Here are some examples of API calls:\n[3] Here are some examples of API calls:\n[4] Here are some examples of API calls:\n[5] Figure 2: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we first sample a position i and corresponding12 API call candidates ci , ci , .\n[6] to make sure that API calls happen close to where the information provided by the API is actually helpful for the model.\n[7] Adding API calls comes without a cost in terms of perplexity for language modeling without any API calls.","system":"gpt_retrieval"},{"id":18,"question":"How does the addition of calls to a Question Answering API help in completing the text and what are some examples of API calls that can be made?","answer":"The addition of calls to a Question Answering API helps in completing the text by providing relevant information to fill in the missing parts of the text [1]. For example, given the input text \"Joe Biden was born in Scranton, Pennsylvania,\" the output text could be \"Joe Biden was born in [QA(\"Where was Joe Biden born?\")],\" where the API call to the Question Answering tool provides the missing information about Joe Biden's birthplace [2]. Other examples of API calls that can be made include \"What other name is Coca-Cola known by?\" and \"Who manufactures Coca-Cola?\" [3].\n\nSources:\n[1] We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.\n[2] Input: Joe Biden was born in Scranton, Pennsylvania.\n[3] Output: Coca-Cola, or [QA(\"What other name is Coca-Cola known by?\")]"},{"id":20,"question":"How did they fine-tune the model?","answer":"The paper discusses a language model called Toolformer, which is based on a pretrained GPT-J model with 6.7B parameters. They conducted experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model. The way these models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be used. However, Toolformer clearly outperforms these baseline models, improving upon the best baseline by 11.7, 5.2 and 18.6 points, respectively. The paper also shows that the ability to leverage the provided tools only emerges at around 775M parameters: smaller models achieve similar performance both with and without tools. Therefore, it can be inferred that they fine-tuned the Toolformer model by prompting it in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be used.\n\nSources:\n[0] Crucially, Toolformer clearly outperforms these baseline models, improving upon the best baseline by 11.7, 5.2 and 18.6 points, respectively.\n[1] Figure 1: Exemplary predictions of Toolformer.\n[2] Figure (<>)4 shows that the ability to leverage the provided tools only emerges at around 775M parameters: smaller models achieve similar performance both with and without tools.\n[3] We conduct experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model ((<>)Wang and Komat-(<>)suzaki, (<>)2021) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model ((<>)Brown et al., (<>)2020) and\n[4] The way these models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision ((<>)Komeili et al., (<>)2022; (<>)Nakano et al., (<>)2021; (<>)Thoppilan et al., (<>)2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be"},{"id":21,"question":"How did they fine-tune the model?","answer":"The model was fine-tuned on a new dataset, C∗, generated by merging API calls for different tools after filtering [1] [2]. This was done in three steps, the first of which involved exploiting the in-context learning ability of the language model (LM) to sample a large number of potential API calls [3]. The second step involved filtering the sampled calls for all APIs [4] [5]. Finally, the LM was fine-tuned on the remaining API calls using a standard language modeling objective [6] [7] [8]. The authors used a subset of CCNet as their language modeling dataset C and GPTJ as their language model M [9] [10]. To reduce the computational cost of annotating C with API calls, the authors defined heuristics for some APIs to get a subset of C for which API calls are more likely to be helpful than for an average text [11] [12]. The model was fine-tuned using a batch size of 128 and a learning rate of 1 · 10−5 with linear warmup for the first 10% of training [13].\n\nSources:\n[1] After filtering, we merge API calls for different tools, resulting in the augmented dataset C∗ , and finetune\n[2] Model Finetuning After sampling and filtering calls for all APIs, we finally merge the remaining API calls and interleave them with the original inputs.\n[3] This is done in three steps, illustrated in Figure (<>)2: First, we exploit the in-context learning ability of M to sample a large number of potential API calls.\n[4] Dataset Generation Throughout all of our experiments, we use a subset of CCNet ((<>)Wenzek et al., (<>)2020) as our language modeling dataset C and GPTJ ((<>)Wang and Komatsuzaki, (<>)2021) as our language model M. To reduce the computational cost of annotating C with API calls, we define heuristics for some APIs to get a subset of C for which API calls are more likely to be helpful than for an average text.\n[5] Model Finetuning After sampling and filtering calls for all APIs, we finally merge the remaining API calls and interleave them with the original inputs.\n[6] Finally, we finetune the LM itself on the API calls that it considers useful.\n[7] We use this new dataset to finetune M, using a standard language modeling objective.\n[8] We use this new dataset to finetune M, using a standard language modeling objective.\n[9] Dataset Generation Throughout all of our experiments, we use a subset of CCNet ((<>)Wenzek et al., (<>)2020) as our language modeling dataset C and GPTJ ((<>)Wang and Komatsuzaki, (<>)2021) as our language model M. To reduce the computational cost of annotating C with API calls, we define heuristics for some APIs to get a subset of C for which API calls are more likely to be helpful than for an average text.\n[10] Dataset Generation Throughout all of our experiments, we use a subset of CCNet ((<>)Wenzek et al., (<>)2020) as our language modeling dataset C and GPTJ ((<>)Wang and Komatsuzaki, (<>)2021) as our language model M. To reduce the computational cost of annotating C with API calls, we define heuristics for some APIs to get a subset of C for which API calls are more likely to be helpful than for an average text.\n[11] Dataset Generation Throughout all of our experiments, we use a subset of CCNet ((<>)Wenzek et al., (<>)2020) as our language modeling dataset C and GPTJ ((<>)Wang and Komatsuzaki, (<>)2021) as our language model M. To reduce the computational cost of annotating C with API calls, we define heuristics for some APIs to get a subset of C for which API calls are more likely to be helpful than for an average text.\n[12] Dataset Generation Throughout all of our experiments, we use a subset of CCNet ((<>)Wenzek et al., (<>)2020) as our language modeling dataset C and GPTJ ((<>)Wang and Komatsuzaki, (<>)2021) as our language model M. To reduce the computational cost of annotating C with API calls, we define heuristics for some APIs to get a subset of C for which API calls are more likely to be helpful than for an average text.\n[13] Model Finetuning We finetune M on C∗ using a batch size of 128 and a learning rate of 1 · 10−5 with linear warmup for the first 10% of training."},{"id":22,"question":"What is the core approach?","answer":"The core approach is to investigate whether the approach enables a model to use tools without any further supervision and to decide for itself when and how to call which of the available tools. This approach enables LMs to learn how to use a variety of tools in a self-supervised way, which is not tied to specific tasks. However, there are some clear limitations to what can be achieved with this method in its current form.\n\nSources:\n[0] In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to specific tasks.\n[1] We investigate whether our approach enables a model to use tools without any further supervision and to decide for itself when and how to call which of the available tools."},{"id":23,"question":"What is the core approach?","answer":"The core approach is the development of Toolformer, a language model that learns to use different tools such as search engines, calculators, and translation systems via simple API calls in a self-supervised way [1] [2] [3]. This is done by finetuning on a large number of sampled API calls that are filtered based on whether they help the model in predicting future tokens [4]. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities [5]. The approach is based on the recent idea of using large LMs with in-context learning to generate entire datasets from scratch [6].\n\nSources:\n[1] We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.\n[2] Our approach for achieving these goals is based on the recent idea of using large LMs with in-context learning ((<>)Brown et al., (<>)2020) to generate entire datasets from scratch ((<>)Schick and Schütze, (<>)2021b; (<>)Honovich et al., (<>)2022; (<>)Wang et al., (<>)2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls.\n[3] We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls.\n[4] We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens.\n[5] Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n[6] Our approach for achieving these goals is based on the recent idea of using large LMs with in-context learning ((<>)Brown et al., (<>)2020) to generate entire datasets from scratch ((<>)Schick and Schütze, (<>)2021b; (<>)Honovich et al., (<>)2022; (<>)Wang et al., (<>)2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls."},{"id":24,"question":"Did they use any models besides GPT-3?","answer":"Yes, they used GPT-J models without tool use, OPT (66B), and Toolformer which is based on GPT-J and mostly relies on the Wikipedia search API to find relevant information. However, it is not clear if they used any other models besides these.\n\nSources:\n[0] GPT-J: A regular GPT-J model without any finetuning.\n[1] It also clearly outperforms OPT (66B) and GPT-3 (175B), despite both models being much larger.\n[2] All GPT-J models without tool use achieve similar performance."},{"id":25,"question":"Did they use any models besides GPT-3?","answer":"Yes, they used several models besides GPT-3, including GPT-J, OPT, and GPT-36 [1] [2] [3] [4]. They also applied their approach to four smaller models from the GPT-2 family [5]. Additionally, they used several other models for different purposes, such as Atlas for few-shot learning [6], Blenderbot 3 for conversational agents [7], and Lamda for dialog applications [8].\n\nSources:\n[1] We conduct experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model ((<>)Wang and Komat-(<>)suzaki, (<>)2021) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model ((<>)Brown et al., (<>)2020) and\n[2] For most tasks, we additionally compare to OPT (66B) ((<>)Zhang et al., (<>)2022) and GPT-36 (<>)(175B) ((<>)Brown et al., (<>)2020), two models that are about 10 and 25 times larger than our other baseline models, respectively.\n[3] Crucially, Toolformer clearly outperforms these baseline models, improving upon the best baseline by 11.7, 5.2 and 18.6 points, respectively.\n[4] It also clearly outperforms OPT (66B) and GPT-3 (175B), despite both models being much larger.\n[5] To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family ((<>)Radford et al., (<>)2019), with 124M, 355M, 775M and 1.6B parameters, respectively.\n[6] (<https://doi.org/10.48550/ARXIV.2208.03299>)Atlas: Few-shot learning with retrieval (<https://doi.org/10.48550/ARXIV.2208.03299>)augmented language models.\n[7] (<https://doi.org/10.48550/ARXIV.2208.03188>)Blenderbot 3: a de-(<https://doi.org/10.48550/ARXIV.2208.03188>)ployed conversational agent that continually learns (<https://doi.org/10.48550/ARXIV.2208.03188>)to responsibly engage.\n[8] (<https://doi.org/10.48550/ARXIV.2201.08239>)Lamda: Lan-(<https://doi.org/10.48550/ARXIV.2201.08239>)guage models for dialog applications."}]}}