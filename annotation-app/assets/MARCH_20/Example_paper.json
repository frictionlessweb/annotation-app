{"title":"Example_paper","questions":{"topicTask":[{"id":0,"text":"Gender Bias"},{"id":1,"text":"Natural Language Processing"},{"id":2,"text":"Representation Bias"},{"id":3,"text":"Word Embeddings"},{"id":4,"text":"Gender Subspace"}],"questionTask":[{"id":5,"text":"What is the goal of designing Gender Bias Evaluation Tests (GBETs) in NLP?"},{"id":10,"text":"What are some methods proposed to mitigate gender bias in word embeddings?"},{"id":15,"text":"How can data augmentation reduce differences in predictions between male and female in hate speech detection?"}],"statementsTask":[{"id":7,"text":"Gender Bias Evaluation Tests (GBETs) are designed to evaluate the level of gender bias present in Natural Language Processing (NLP) models. The goal of designing GBETs is to identify and mitigate gender bias in NLP models, which can lead to unfair treatment of individuals based on their gender. GBETs can help improve the accuracy and fairness of NLP models by identifying and addressing gender bias in the training data and algorithms used to develop these models. By using GBETs, researchers and developers can ensure that NLP models are not perpetuating harmful stereotypes or discriminating against individuals based on their gender.\n\nSources:","system":"gpt_retrieval"},{"id":9,"text":"The goal of designing Gender Bias Evaluation Tests (GBETs) in NLP is to provide standardized metrics that rigorously measure the gender bias in NLP applications [1]. GBETs use differences in values of the probing concept or prediction accuracies relating to the probing concept between gender-swapped data points to measure bias [2]. By designing test sets, measuring performance differences between genders reveals representational gender bias in the context of recognition, stereotyping, and under-representation [3]. Additionally, systems that make biased predictions may discourage minorities from using those systems and having their data collected, thus worsening the disparity in the data sets [4].\n\nSources:\n[1] The goal of designing GBETs is to provide\n[2] GBETs use differences in values of the probing concept or prediction accuracies relating to the probing concept between gender-swapped data points to measure bias.\n[3] By designing test sets, measuring performance differences between genders reveals representational gender bias in the context of recognition, stereotyping, and under-representation.\n[4] Additionally, systems that make biased predictions may discourage minorities from using those systems and having their data collected, thus worsening the disparity in the data sets ((<>)Hashimoto et al., (<>)2018)."},{"id":12,"text":"Gender bias in word embeddings can be mitigated through various methods. One approach is to debias the embeddings by removing the gender component from the embeddings. This can be done by neutralizing the gender subspace, which involves identifying the gender direction and projecting the embeddings onto the orthogonal complement of this direction. Another approach is to equalize the embeddings, which involves modifying the embeddings to ensure that words with the same meaning but different genders have the same distance from the gender-neutral word. Additionally, some researchers have proposed using counterfactual data augmentation to mitigate gender bias in word embeddings. This involves generating new training data by swapping the gender of words in existing sentences. Finally, it is important to note that while these methods can help mitigate gender bias in word embeddings, they are not foolproof and may not completely eliminate all forms of bias.\n\nSources:","system":"gpt_retrieval"},{"id":14,"text":"Several methods have been proposed to mitigate gender bias in word embeddings. One approach is to identify the gender subspace by building a linear support vector machine to classify words into gender-specific and gender-neutral words [1]. However, this method fails to capture the full picture of gender bias in vector spaces [2]. Another approach is to remove the projections of word embeddings representing gender-neutral words onto the gender subspace, but word embeddings representing words with similar biases still cluster together [3]. Some methods adjust predictions in NLP systems to reduce bias amplification [4] [5] [6]. Other methods surgically alter the embedding space by removing the gender component only from gender-neutral words [7] [8] [9] [10]. Finally, some methods use adversarial learning to mitigate unwanted biases [11] [12]. However, it is noted that most gender debiasing methods have only been empirically verified in limited applications, and it is not clear that these methods can generalize to other tasks or models [13].\n\nSources:\n[1] To identify the gender subspace, they first build a linear support vector machine to classify words into a set of gender-specific and a set of gender-neutral words based on a training set of hand-selected gender-specific words.\n[2] However, (<>)Gonen and Goldberg ((<>)2019) note that the above method fails to capture the full picture of gender bias in vector spaces.\n[3] Specifically, even after the projections of word embeddings representing gender-neutral words onto the gender subspace have been removed, word embeddings representing words with similar biases still cluster together.\n[4] Some gender debiasing methods in NLP adjust predictions in NLP systems.\n[5] ((<>)2017) show that an NLP model risks amplifying bias by making predictions which exacerbate biases present in the training set.\n[6] ((<>)2017) proposed Reducing Bias Amplification (RBA) based on a constrained conditional model ((<>)Roth and Yih, (<>)2004), which takes an existing model’s optimization function and constrains that function to ensure its predictions fit defined conditions.\n[7] ((<>)2016) build upon (<>)Schmidt ((<>)2015) and propose to surgerically alter the embedding space by removing the gender component only from gender-neutral words.\n[8] ((<>)2018b) propose a new method called GN-GloVe that does not use a classifier to create a set of gender-specific words.\n[9] The authors train the word embeddings by isolating gender information in specific dimensions and maintaining gender-neutral information in the other dimensions.\n[10] They do this by (1) minimizing the negative difference (i.e.\n[11] Mitigating Unwanted Biases with Adversarial Learning.\n[12] Mitigating Unwanted Biases with Adversarial Learning.\n[13] Second, most gender debiasing methods have only been empirically verified in limited applications ((<>)Zhang et al., (<>)2018; (<>)Zhao et al., (<>)2017), and it is not clear that these methods can generalize to other tasks or models."},{"id":17,"text":"Data augmentation can reduce differences in predictions between male and female in hate speech detection by increasing the amount of training data available for the model to learn from. This can help to balance out any biases that may exist in the original dataset. For example, if the original dataset contains more examples of hate speech directed towards one gender than the other, the model may be more accurate at detecting hate speech towards that gender. However, by augmenting the data to create more balanced examples, the model can learn to detect hate speech towards both genders equally well. Additionally, data augmentation techniques such as random cropping and flipping can help to increase the robustness of the model to variations in the input data. This can help to reduce the impact of any gender-specific features that may be present in the input data.\n\nSources:","system":"gpt_retrieval"},{"id":19,"text":"According to [1] and [2], data augmentation can significantly reduce FNED and FPED differences between male and female predictions of a Convolutional Neural Network in hate speech detection. The process of data augmentation involves creating gender-swapped equivalents of every sentence in the original dataset [3]. This technique has been used to mitigate unwanted biases in text classification [4], reduce gender bias in abusive language detection [5][6], and improve fairness and calibration in natural language processing [7][8][9]. Adversarial learning has also been used to mitigate unwanted biases in text classification [10][11].\n\nSources:\n[1] In hate speech detection, data augmentation reduced FNED and FPED differences between male and female predictions of a Convolutional Neural Network by a wide margin ((<>)Park et al., (<>)2018).\n[2] In hate speech detection, data augmentation reduced FNED and FPED differences between male and female predictions of a Convolutional Neural Network by a wide margin ((<>)Park et al., (<>)2018).\n[3] Data augmentation works as follows: for every sentence in the original data set, create that sentence’s gender-swapped equivalent using the procedure described in (<>)2.3.\n[4] Measuring and Mitigating Unintended Bias in Text Classification.\n[5] Reducing Gender Bias in Abusive Language Detection.\n[6] Reducing Gender Bias in Abusive Language Detection.\n[7] On Fairness and Calibration.\n[8] On Fairness and Calibration.\n[9] The Glass Ceiling in NLP.\n[10] Mitigating Unwanted Biases with Adversarial Learning.\n[11] Mitigating Unwanted Biases with Adversarial Learning."}],"qaTask":[{"id":6,"question":"What is the goal of designing Gender Bias Evaluation Tests (GBETs) in NLP?","answer":"Gender Bias Evaluation Tests (GBETs) are designed to evaluate the level of gender bias present in Natural Language Processing (NLP) models. The goal of designing GBETs is to identify and mitigate gender bias in NLP models, which can lead to unfair treatment of individuals based on their gender. GBETs can help improve the accuracy and fairness of NLP models by identifying and addressing gender bias in the training data and algorithms used to develop these models. By using GBETs, researchers and developers can ensure that NLP models are not perpetuating harmful stereotypes or discriminating against individuals based on their gender.\n\nSources:","system":"gpt_retrieval"},{"id":8,"question":"What is the goal of designing Gender Bias Evaluation Tests (GBETs) in NLP?","answer":"The goal of designing Gender Bias Evaluation Tests (GBETs) in NLP is to provide standardized metrics that rigorously measure the gender bias in NLP applications [1]. GBETs use differences in values of the probing concept or prediction accuracies relating to the probing concept between gender-swapped data points to measure bias [2]. By designing test sets, measuring performance differences between genders reveals representational gender bias in the context of recognition, stereotyping, and under-representation [3]. Additionally, systems that make biased predictions may discourage minorities from using those systems and having their data collected, thus worsening the disparity in the data sets [4].\n\nSources:\n[1] The goal of designing GBETs is to provide\n[2] GBETs use differences in values of the probing concept or prediction accuracies relating to the probing concept between gender-swapped data points to measure bias.\n[3] By designing test sets, measuring performance differences between genders reveals representational gender bias in the context of recognition, stereotyping, and under-representation.\n[4] Additionally, systems that make biased predictions may discourage minorities from using those systems and having their data collected, thus worsening the disparity in the data sets ((<>)Hashimoto et al., (<>)2018)."},{"id":11,"question":"What are some methods proposed to mitigate gender bias in word embeddings?","answer":"Gender bias in word embeddings can be mitigated through various methods. One approach is to debias the embeddings by removing the gender component from the embeddings. This can be done by neutralizing the gender subspace, which involves identifying the gender direction and projecting the embeddings onto the orthogonal complement of this direction. Another approach is to equalize the embeddings, which involves modifying the embeddings to ensure that words with the same meaning but different genders have the same distance from the gender-neutral word. Additionally, some researchers have proposed using counterfactual data augmentation to mitigate gender bias in word embeddings. This involves generating new training data by swapping the gender of words in existing sentences. Finally, it is important to note that while these methods can help mitigate gender bias in word embeddings, they are not foolproof and may not completely eliminate all forms of bias.\n\nSources:","system":"gpt_retrieval"},{"id":13,"question":"What are some methods proposed to mitigate gender bias in word embeddings?","answer":"Several methods have been proposed to mitigate gender bias in word embeddings. One approach is to identify the gender subspace by building a linear support vector machine to classify words into gender-specific and gender-neutral words [1]. However, this method fails to capture the full picture of gender bias in vector spaces [2]. Another approach is to remove the projections of word embeddings representing gender-neutral words onto the gender subspace, but word embeddings representing words with similar biases still cluster together [3]. Some methods adjust predictions in NLP systems to reduce bias amplification [4] [5] [6]. Other methods surgically alter the embedding space by removing the gender component only from gender-neutral words [7] [8] [9] [10]. Finally, some methods use adversarial learning to mitigate unwanted biases [11] [12]. However, it is noted that most gender debiasing methods have only been empirically verified in limited applications, and it is not clear that these methods can generalize to other tasks or models [13].\n\nSources:\n[1] To identify the gender subspace, they first build a linear support vector machine to classify words into a set of gender-specific and a set of gender-neutral words based on a training set of hand-selected gender-specific words.\n[2] However, (<>)Gonen and Goldberg ((<>)2019) note that the above method fails to capture the full picture of gender bias in vector spaces.\n[3] Specifically, even after the projections of word embeddings representing gender-neutral words onto the gender subspace have been removed, word embeddings representing words with similar biases still cluster together.\n[4] Some gender debiasing methods in NLP adjust predictions in NLP systems.\n[5] ((<>)2017) show that an NLP model risks amplifying bias by making predictions which exacerbate biases present in the training set.\n[6] ((<>)2017) proposed Reducing Bias Amplification (RBA) based on a constrained conditional model ((<>)Roth and Yih, (<>)2004), which takes an existing model’s optimization function and constrains that function to ensure its predictions fit defined conditions.\n[7] ((<>)2016) build upon (<>)Schmidt ((<>)2015) and propose to surgerically alter the embedding space by removing the gender component only from gender-neutral words.\n[8] ((<>)2018b) propose a new method called GN-GloVe that does not use a classifier to create a set of gender-specific words.\n[9] The authors train the word embeddings by isolating gender information in specific dimensions and maintaining gender-neutral information in the other dimensions.\n[10] They do this by (1) minimizing the negative difference (i.e.\n[11] Mitigating Unwanted Biases with Adversarial Learning.\n[12] Mitigating Unwanted Biases with Adversarial Learning.\n[13] Second, most gender debiasing methods have only been empirically verified in limited applications ((<>)Zhang et al., (<>)2018; (<>)Zhao et al., (<>)2017), and it is not clear that these methods can generalize to other tasks or models."},{"id":16,"question":"How can data augmentation reduce differences in predictions between male and female in hate speech detection?","answer":"Data augmentation can reduce differences in predictions between male and female in hate speech detection by increasing the amount of training data available for the model to learn from. This can help to balance out any biases that may exist in the original dataset. For example, if the original dataset contains more examples of hate speech directed towards one gender than the other, the model may be more accurate at detecting hate speech towards that gender. However, by augmenting the data to create more balanced examples, the model can learn to detect hate speech towards both genders equally well. Additionally, data augmentation techniques such as random cropping and flipping can help to increase the robustness of the model to variations in the input data. This can help to reduce the impact of any gender-specific features that may be present in the input data.\n\nSources:","system":"gpt_retrieval"},{"id":18,"question":"How can data augmentation reduce differences in predictions between male and female in hate speech detection?","answer":"According to [1] and [2], data augmentation can significantly reduce FNED and FPED differences between male and female predictions of a Convolutional Neural Network in hate speech detection. The process of data augmentation involves creating gender-swapped equivalents of every sentence in the original dataset [3]. This technique has been used to mitigate unwanted biases in text classification [4], reduce gender bias in abusive language detection [5][6], and improve fairness and calibration in natural language processing [7][8][9]. Adversarial learning has also been used to mitigate unwanted biases in text classification [10][11].\n\nSources:\n[1] In hate speech detection, data augmentation reduced FNED and FPED differences between male and female predictions of a Convolutional Neural Network by a wide margin ((<>)Park et al., (<>)2018).\n[2] In hate speech detection, data augmentation reduced FNED and FPED differences between male and female predictions of a Convolutional Neural Network by a wide margin ((<>)Park et al., (<>)2018).\n[3] Data augmentation works as follows: for every sentence in the original data set, create that sentence’s gender-swapped equivalent using the procedure described in (<>)2.3.\n[4] Measuring and Mitigating Unintended Bias in Text Classification.\n[5] Reducing Gender Bias in Abusive Language Detection.\n[6] Reducing Gender Bias in Abusive Language Detection.\n[7] On Fairness and Calibration.\n[8] On Fairness and Calibration.\n[9] The Glass Ceiling in NLP.\n[10] Mitigating Unwanted Biases with Adversarial Learning.\n[11] Mitigating Unwanted Biases with Adversarial Learning."},{"id":20,"question":"What are the various debiasing methods discussed in this document?","answer":"The document discusses several debiasing methods, including awareness and education, decision-making strategies, diversity and inclusion, and technology-based solutions. Awareness and education involve recognizing and acknowledging biases and their impact. Decision-making strategies include techniques such as slowing down the decision-making process and considering multiple perspectives. Diversity and inclusion involve creating a more diverse and inclusive environment to reduce bias. Technology-based solutions include using algorithms and artificial intelligence to reduce bias in decision-making.\n\nSources:"},{"id":21,"question":"What are the various debiasing methods discussed in this document?","answer":"The document discusses various debiasing methods in NLP, including name anonymization, gender tagging, bias fine-tuning, gender swapping, genderless word embeddings, surgically altered embedding space, constrained conditional model, generative adversarial network, and more [1] [2] [3] [4] [5] [6] [7] [8] [9] [10]. The document also notes the limitations of current approaches, such as focusing on a single modular process, limited empirical verification, and potential noise introduction [11] [12] [13]. The document suggests future directions for mitigating gender bias in languages beyond English, considering non-binary genders and racial biases, and interdisciplinary discussions between computer scientists and sociologists [14] [15] [16] [17] [18] [19].\n\nSources:\n[1] Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods.\n[2] word embeddings), and algorithms themselves ((<>)Zhao et al., (<>)2018a; (<>)Boluk-(<>)basi et al., (<>)2016; (<>)Caliskan et al., (<>)2017; (<>)Garg et al., (<>)2018).\n[3] Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods.\n[4] To mitigate this, (<>)Zhao (<>)et al.\n[5] The identification of gender-specific words and their equivalent opposite gender word requires lists typically created by crowd workers.\n[6] Bias fine-tuning incorporates transfer learning from an unbiased data set to ensure that a model contains minimal bias before fine-tuning the model on a more biased data set used to train for the target task directly ((<>)Park et al., (<>)2018).\n[7] ((<>)2018) use transfer learning from a gender unbiased abusive tweets data set ((<>)Founta et al., (<>)2018) and fine-tuning on a gender-biased sexist tweets data set ((<>)Waseem and Hovy, (<>)2016) to train a Convolutional Neural Network (CNN).\n[8] (<>)Schmidt ((<>)2015) first removed similarity to the gender subspace in word embeddings by building a genderless framework using cosine similarity and orthogonal vectors ((<>)Schmidt, (<>)2015).\n[9] ((<>)2017) proposed Reducing Bias Amplification (RBA) based on a constrained conditional model ((<>)Roth and Yih, (<>)2004), which takes an existing model’s optimization function and constrains that function to ensure its predictions fit defined conditions.\n[10] ((<>)2018) propose a variation on the traditional generative adversarial network ((<>)Goodfel-(<>)low et al., (<>)2014) by having the generator learn with respect to a protected gender attribute.\n[11] We note the following limitations of current approaches.\n[12] First, the majority of debi-asing techniques focus on a single, modular process of an end-to-end NLP system.\n[13] Second, most gender debiasing methods have only been empirically verified in limited applications ((<>)Zhang et al., (<>)2018; (<>)Zhao et al., (<>)2017), and it is not clear that these methods can generalize to other tasks or models.\n[14] Below, we identify a few future directions.\n[15] With few exceptions ((<>)Manzini et al., (<>)2019), work on debiasing in NLP has assumed that the protected attribute being discriminated against is binary.\n[16] Additionally, mitigating gender bias in NLP is both a sociological and an engineering problem.\n[17] To completely debias effectively, it is important to understand how machine learning methods encode biases and how humans perceive biases.\n[18] Approaches from other technical fields may improve current debiasing methods in NLP or inspire the development of new, more effective methods even if the properties of the data or\n[19] Discussions between computer scientists and sociologists may improve understanding of latent gender bias found in machine learning data sets and model predictions."},{"id":22,"question":"What are the pros and cons of data augmentation?","answer":"Data augmentation is a technique used in machine learning to increase the size of a dataset by creating new data from the existing data. The pros of data augmentation are that it can improve the accuracy of a model by reducing overfitting, it can help to balance an imbalanced dataset, and it can reduce the need for collecting more data. However, the cons of data augmentation are that it can increase the risk of introducing noise into the dataset, it can be time-consuming and computationally expensive, and it can lead to a decrease in the interpretability of the model. Overall, data augmentation can be a useful technique for improving the performance of a machine learning model, but it should be used with caution and with an understanding of its potential drawbacks.\n\nSources:"},{"id":23,"question":"What are the pros and cons of data augmentation?","answer":"Data augmentation is a flexible method that can mitigate gender bias in several different tasks [1]. It is easy to implement, but creating the annotated list can be expensive if there is high variability in the data or if the data set is large since more annotations will be required [2] [3]. Gender-swapping and name anonymization are two common techniques used in data augmentation [4]. However, blindly gender-swapping can create nonsensical sentences [5]. Data augmentation has been shown to be effective in reducing gender bias in hate speech detection [6] and coreference resolution models [7]. It has also been found to be more effective than bias fine-tuning [8]. However, in some tasks like machine translation, confounding the gender of the source of a data point can lead to inaccurate predictions [9]. While existing methods successfully mitigate bias with respect to projection onto the gender subspace in some degrees, gender bias based on more subtle metrics such as cluster bias still exist [10].\n\nSources:\n[1] Data augmentation has been shown to be flexible; it can mitigate gender bias in several differ-\n[2] Data augmentation is easy to implement, but creating the annotated list can be expensive if there is high variability in the data or if the data set is large since more annotations will be required.\n[3] Data augmentation is easy to implement, but creating the annotated list can be expensive if there is high variability in the data or if the data set is large since more annotations will be required.\n[4] For instance, Mary likes her mother Jan becomes E1 likes his father E2 after applying gender-swapping and name anonymization for data augmentation.\n[5] Lastly, blindly gender-swapping can create nonsensical sentences – for example, gender-swapping “she gave birth” to “he gave birth” ((<>)Madaan et al., (<>)2018).\n[6] In hate speech detection, data augmentation reduced FNED and FPED differences between male and female predictions of a Convolutional Neural Network by a wide margin ((<>)Park et al., (<>)2018).\n[7] When applied to a neural network based coreference resolution model ((<>)Lee et al., (<>)2017, (<>)2018) originally trained on OntoNotes 5.0 which was tested on WinoBias, gender augmentation lowered the difference between F1 scores on pro-stereotypical and anti-stereotypical test sets significantly, which indicates the model was less inclined to make gender-biased predictions ((<>)Zhao et al., (<>)2018a, (<>)2019).\n[8] ((<>)2018) find that gender-swapping was more effective at both removing bias and retaining performance than bias fine-tuning.\n[9] In some tasks, like Machine Translation (MT), confounding the gender of the source of a data point can lead to inaccurate predictions.\n[10] While existing methods successfully mitigate bias with respect to projection onto the gender subspace in some degrees, (<>)Gonen and Gold-(<>)berg ((<>)2019) show that gender bias based on more subtle metrics such as cluster bias still exist."},{"id":24,"question":"What are the limitations of the bias measurement techniques that are discussed?","answer":"The sources provided do not discuss any bias measurement techniques or their limitations. Therefore, I cannot provide an answer to this question.\n\nSources:"},{"id":25,"question":"What are the limitations of the bias measurement techniques that are discussed?","answer":"The limitations of the bias measurement techniques discussed include the fact that standard evaluation data sets in NLP are inadequate for measuring gender bias [1], and that these data sets often contain biases themselves [2]. Additionally, predictions made by systems performing complex NLP tasks depend on many factors, so carefully designed data sets are necessary to isolate the effect of gender on the output in order to probe gender bias [3]. Blindly gender-swapping can also create nonsensical sentences [4]. Data augmentation is easy to implement, but creating the annotated list can be expensive if there is high variability in the data or if the data set is large since more annotations will be required [5]. Gender-tagging can also be expensive, as knowing the gender of the source of a data point requires meta-information, and obtaining this could be costly in terms of memory usage and time [6]. Finally, while existing methods successfully mitigate bias with respect to projection onto the gender subspace in some degrees, gender bias based on more subtle metrics such as cluster bias still exist [7] [8]. It is also noted that debiasing word embeddings can blind an AI agent's perception rather than teaching it to perform fair actions [9]. Furthermore, most gender debiasing methods have only been empirically verified in limited applications, and it is not clear that these methods can generalize to other tasks or models [10] [11]. Finally, it remains to be discovered how individual parts of an end-to-end NLP system harmonize together to form an ideally unbiased system [12] [13] [14] [15]. [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30].\n\nSources:\n[1] Standard evaluation data sets in NLP are inadequate for measuring gender bias.\n[2] For one, these data sets often also contain biases (such as containing more male references than female references), so evaluation on them might not reveal gender bias.\n[3] Furthermore, predictions made by systems performing complex NLP tasks depend on many factors; we must carefully design data sets to isolate the effect of gender of the output in order to be able to probe gender bias.\n[4] Lastly, blindly gender-swapping can create nonsensical sentences – for example, gender-swapping “she gave birth” to “he gave birth” ((<>)Madaan et al., (<>)2018).\n[5] Data augmentation is easy to implement, but creating the annotated list can be expensive if there is high variability in the data or if the data set is large since more annotations will be required.\n[6] However, gender-tagging can be expensive: knowing the gender of the source of a data point requires meta-information, and obtaining this could be costly in terms of memory usage and time.\n[7] Specifically, even after the projections of word embeddings representing gender-neutral words onto the gender subspace have been removed, word embeddings representing words with similar biases still cluster together.\n[8] While existing methods successfully mitigate bias with respect to projection onto the gender subspace in some degrees, (<>)Gonen and Gold-(<>)berg ((<>)2019) show that gender bias based on more subtle metrics such as cluster bias still exist.\n[9] ((<>)2017) argue that debiasing word embeddings blinds an AI agent’s perception rather than teaching it to perform fair actions.\n[10] Second, most gender debiasing methods have only been empirically verified in limited applications ((<>)Zhang et al., (<>)2018; (<>)Zhao et al., (<>)2017), and it is not clear that these methods can generalize to other tasks or models.\n[11] Gender Bias in Contextualized Word Embeddings.\n[12] We note the following limitations of current approaches.\n[13] First, the majority of debi-asing techniques focus on a single, modular process of an end-to-end NLP system.\n[14] It remains to be discovered how these individual parts harmonize together to form an ideally unbiased system.\n[15] Gender debiasing methods in NLP are not sufficient to debias models end-to-end for many applications.\n[16] Specifically, even after the projections of word embeddings representing gender-neutral words onto the gender subspace have been removed, word embeddings representing words with similar biases still cluster together.\n[17] Standard evaluation data sets in NLP are inadequate for measuring gender bias.\n[18] For one, these data sets often also contain biases (such as containing more male references than female references), so evaluation on them might not reveal gender bias.\n[19] Furthermore, predictions made by systems performing complex NLP tasks depend on many factors; we must carefully design data sets to isolate the effect of gender of the output in order to be able to probe gender bias.\n[20] Lastly, blindly gender-swapping can create nonsensical sentences – for example, gender-swapping “she gave birth” to “he gave birth” ((<>)Madaan et al., (<>)2018).\n[21] Data augmentation is easy to implement, but creating the annotated list can be expensive if there is high variability in the data or if the data set is large since more annotations will be required.\n[22] However, gender-tagging can be expensive: knowing the gender of the source of a data point requires meta-information, and obtaining this could be costly in terms of memory usage and time.\n[23] While existing methods successfully mitigate bias with respect to projection onto the gender subspace in some degrees, (<>)Gonen and Gold-(<>)berg ((<>)2019) show that gender bias based on more subtle metrics such as cluster bias still exist.\n[24] ((<>)2017) argue that debiasing word embeddings blinds an AI agent’s perception rather than teaching it to perform fair actions.\n[25] We note the following limitations of current approaches.\n[26] Second, most gender debiasing methods have only been empirically verified in limited applications ((<>)Zhang et al., (<>)2018; (<>)Zhao et al., (<>)2017), and it is not clear that these methods can generalize to other tasks or models.\n[27] First, the majority of debi-asing techniques focus on a single, modular process of an end-to-end NLP system.\n[28] It remains to be discovered how these individual parts harmonize together to form an ideally unbiased system.\n[29] Gender debiasing methods in NLP are not sufficient to debias models end-to-end for many applications.\n[30] Gender Bias in Contextualized Word Embeddings."}]},"pdf_url":"/api/v1/static/MARCH_20/Example_paper.pdf"}