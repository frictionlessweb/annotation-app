{"title":"2212_09746","questions":{"topicTask":[{"id":0,"text":"Human-LM interaction"},{"id":1,"text":"Non-interactive evaluation"},{"id":2,"text":"Interaction traces"},{"id":3,"text":"System logic"},{"id":4,"text":"Third-party perspectives"}],"questionTask":[{"id":5,"text":"What are the five tasks and human-LM interactions shown in Figure 2?"},{"id":10,"text":"How was the performance of different text summarization models evaluated?"},{"id":15,"text":"What is the purpose of the survey question about changing interaction with the AI Teammate over the course of solving the crossword puzzle?"}],"statementsTask":[{"id":7,"text":"Figure 2 shows the five tasks studied in the paper for human-LM interaction: social dialogue, question answering, crossword puzzles, text summarization, and metaphor generation. For each task, a system was built to facilitate human-LM interaction in the context of the task. The paper also defines a set of metrics for evaluating human-LM interaction across these tasks.\n\nSources:\n[0] Figure 2: Five tasks and human-LM interaction in the context of accomplishing the tasks.","system":"gpt_retrieval"},{"id":9,"text":"Figure 2 shows five tasks and human-LM interactions: social dialogue, question answering, crossword puzzles, text summarization, and metaphor generation [1] [2] [3] [4].\n\nSources:\n[1] Figure 2: Five tasks and human-LM interaction in the context of accomplishing the tasks.\n[2] We then design five tasks ranging from goal-oriented to open-ended to capture different forms of interaction.\n[3] Figure (<>)2 shows five tasks we study in this paper: social dialogue, question answering, crossword puzzles, text summarization, and metaphor generation (see Section (<>)3 for a detailed description of each task).\n[4] Figure (<>)2 shows five tasks we study in this paper: social dialogue, question answering, crossword puzzles, text summarization, and metaphor generation (see Section (<>)3 for a detailed description of each task)."},{"id":12,"text":"The performance of different text summarization models was evaluated based on automatic metrics and third-party evaluation. Third-party evaluators were asked to evaluate the quality of summaries with respect to their consistency, relevancy, and coherence. Original summaries from Text-Babbage were rated as the most consistent and relevant among the four models based on third-party evaluation. Model performance was also evaluated based on user survey responses. However, there is a discrepancy between the metrics commonly used to evaluate summarization quality and what users find helpful in interacting with and improving these models. Text summarization is a well-established research direction in NLP, where the goal is to compress the salient information in the source document into a coherent and fluent summary.\n\nSources:\n[0] This observation reveals a discrepancy between the metrics commonly used to evaluate summarization quality and what users find helpful in interacting with and improving these models.\n[1] Table 6: [Text summarization] Third-party evaluation of model-generated summaries.\n[2] However, from third-party evaluation, original summaries from Text-Babbage were rated as the most consistent and relevant among the four models (Table (<>)6).\n[3] Table 15: [Text summarization] Model performance based on automatic metrics.\n[4] In text summarization, we ask third-party evaluators to evaluate the quality of summaries with respect to their consistency (i.e., all the information in the summary is inferred from the document), relevancy (i.e., the summary includes only important information and no excess information), and coherence (i.e., the summary organizes the relevant information in a well-structured manner).\n[5] Table 16: [Text summarization] Model performance based on user survey responses..","system":"gpt_retrieval"},{"id":14,"text":"The performance of different text summarization models was evaluated using a new framework called Human-AI Language-based Interaction Evaluation (HALIE) [1] [2] [3] [4] [5] [6]. HALIE expands non-interactive evaluation along three dimensions, capturing the interactive process, the first-person subjective experience, and notions of preference beyond quality [7] [8] [9] [10] [11]. Five tasks were designed to capture different forms of interaction [12] [13]. Four state-of-the-art language models (LMs) were evaluated: three variants of OpenAI’s GPT-3 and AI21’s J1-Jumbo [14] [15]. HALIE defines metrics on interaction traces, which are organized along three dimensions: targets, perspectives, and criteria [16]. Targets include more than just the final output, and cover the entire interaction process, while perspectives are not limited to third parties, but the users who interact with LMs [17]. From the 1015 interaction traces collected, it was observed that better non-interactive performance does not always lead to better human-LM interaction [18]. Users edited summaries generated by TextBabbage the most, and TextDavinci the least [19]. Overall, users perceived TextDavinci to be most helpful and better improves over time compared to the other models [20]. However, third-party evaluators considered sentences written with the same model to be the worst among the four models [21].\n\nSources:\n[1] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[2] We develop an evaluation framework, Human-AI Language-based Interaction Evaluation (HALIE) that puts interaction at the center.\n[3] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[4] We propose a framework, HALIE, that expands on non-interactive evaluation along three dimensions: (i) we capture the full process in addition to the final output (targets); (ii) we capture the first-person subjective experience of users interacting with the LM in addition to the perspective of a third-party (perspectives), and (iii) we consider notions of preference beyond quality (criteria).\n[5] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[6] To evaluate human-LM interaction, HALIE defines metrics on interaction traces, which are organized along the following three dimensions (Figure (<>)1)—targets, perspectives, and criteria: (1) targets include more than just the final output, and cover the entire interaction process (e.g., user queries and edits); (2) perspectives are not limited to third parties, but the users who interact with LMs\n[7] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[8] We develop an evaluation framework, Human-AI Language-based Interaction Evaluation (HALIE) that puts interaction at the center.\n[9] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[10] We propose a framework, HALIE, that expands on non-interactive evaluation along three dimensions: (i) we capture the full process in addition to the final output (targets); (ii) we capture the first-person subjective experience of users interacting with the LM in addition to the perspective of a third-party (perspectives), and (iii) we consider notions of preference beyond quality (criteria).\n[11] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[12] We then design five tasks ranging from goal-oriented to open-ended to capture different forms of interaction.\n[13] Figure 2: Five tasks and human-LM interaction in the context of accomplishing the tasks.\n[14] On four state-of-the-art LMs (three variants of OpenAI’s GPT-3 and AI21’s J1-Jumbo), we find that non-interactive performance does not always result in better human-LM interaction and that first-person and third-party metrics can diverge, suggesting the importance of examining the nuances of human-LM interaction.\n[15] With these systems, We evaluated four state-of-the-art LMs: three variants of OpenAI’s GPT-3 ((<>)Brown et al., (<>)2020; (<>)Ouyang et al., (<>)2022)—TextDavinci (text-davinci-001), TextBabbage (text-babbage-001), and Davinci (davinci)— and AI21’s Jurassic-1 ((<>)Lieber et al., (<>)2021)—Jumbo (j1-jumbo).\n[16] To evaluate human-LM interaction, HALIE defines metrics on interaction traces, which are organized along the following three dimensions (Figure (<>)1)—targets, perspectives, and criteria: (1) targets include more than just the final output, and cover the entire interaction process (e.g., user queries and edits); (2) perspectives are not limited to third parties, but the users who interact with LMs\n[17] To evaluate human-LM interaction, HALIE defines metrics on interaction traces, which are organized along the following three dimensions (Figure (<>)1)—targets, perspectives, and criteria: (1) targets include more than just the final output, and cover the entire interaction process (e.g., user queries and edits); (2) perspectives are not limited to third parties, but the users who interact with LMs\n[18] From the 1015 interaction traces we collected, we observe that better non-interactive performance does not always lead to better human-LM interaction (e.g., Section (<>)3.2).\n[19] Table 5: [Text summarization] Users edited summaries generated by TextBabbage the most, and TextDavinci the least.\n[20] Overall, users perceived TextDavinci to be most helpful (helpfulness) and better improves over time (improvement) compared to the other models.\n[21] However, third-party evaluators considered sentences written with the same model to be the worst among the four models (Table (<>)9)."},{"id":17,"text":"The purpose of the survey question about changing interaction with the AI Teammate over the course of solving the crossword puzzle is to closely study engagement in the relatively long period of interaction between users and the AI Teammate in the crossword puzzle task. The survey question is used to measure engagement in two ways and to understand how users chose to interact with the AI Teammate over time. Additionally, the survey question provides insight into how users found the challenge of figuring out how to best use the AI Teammate entertaining. The survey question was included in a survey that each user completed about their experience interacting with the AI Teammate after completing the crossword puzzle. The users were provided with a description of how to interact with the AI Teammate and overall crossword puzzle interface, as well as examples of prompts they could provide to assist with solving puzzle clues. The cumulative queries to the AI Teammate over time for all users were also analyzed to understand how users interacted with the AI Teammate.\n\nSources:\n[0] Example responses to the survey question “Did the way you chose to interact with the AI Teammate change over the course of solving the crossword puzzle?\n[1] For QA and crossword puzzles, we asked users to answer the survey question “Did the way you chose to interact with the AI Teammate change over time?\n[2] The relatively long period of interaction between users and the AI Teammate in the crossword puzzle task provides an opportunity to closely study engagement, which we measure in two ways.\n[3] Change (free-form): Did the way you chose to interact with the AI Teammate change over the course of solving the crossword puzzle?\n[4] Users were provided a description of how to interact with the AI Teammate and overall crossword puzzle interface, as well as examples of prompts they could provide to assist with solving puzzle clues.\n[5] In our setting, users try to complete an entire crossword puzzle with the ability to query an LM-based “AI Teammate” via a chat-based interface.\n[6] Finally, unlike with the use of AI solvers that seek to solve a crossword puzzle perfectly ((<>)Wallace et al., (<>)2022a), some users found the challenge of figuring out how to best use the AI Teammate itself entertaining:\n[7] Afterwards, each user completed a survey about their experience interacting with the AI Teammate.","system":"gpt_retrieval"},{"id":19,"text":"The purpose of the survey question about changing interaction with the AI Teammate over the course of solving the crossword puzzle is to examine user accommodation, or the change in behavior in response to the AI system, both qualitatively and quantitatively [1] [2]. The survey question is used to gather qualitative data on how users adapt their behavior as they learn more about the strengths and weaknesses of the system and the underlying model [3]. The change in query strategies over time is measured quantitatively to analyze how the distribution of prompt type changed for users over time [4] [5]. This analysis helps to understand how these interactions come to affect human language practices, culture, well-being, and broader society [6].\n\nSources:\n[1] For QA and crossword puzzles, we asked users to answer the survey question “Did the way you chose to interact with the AI Teammate change over time?\n[2] We examine user accommodation, or change in behavior in response to the AI system, in two ways: qualitatively, through a free-response survey question, and quantitatively, via measuring the change in query strategies over time.\n[3] One important aspect of human-AI interaction to account for is user accommodation, or the ability of users to adapt their behavior as they learn more about the strengths and weaknesses of a system and the underlying model.\n[4] We examine user accommodation, or change in behavior in response to the AI system, in two ways: qualitatively, through a free-response survey question, and quantitatively, via measuring the change in query strategies over time.\n[5] To further examine these trends quantitatively, we categorized all user prompts using the below taxonomy in order to measure how the distribution of prompt type changed for users over time:\n[6] Overall, especially given the rapid deployment of LMs, including a variety of different interfaces, we recommend that future work should actively monitor how these interactions come to affect human language practices (e.g., writing, reading, listening, speaking), culture, well-being, and broader society."}],"qaTask":[{"id":6,"question":"What are the five tasks and human-LM interactions shown in Figure 2?","answer":"Figure 2 shows the five tasks studied in the paper for human-LM interaction: social dialogue, question answering, crossword puzzles, text summarization, and metaphor generation. For each task, a system was built to facilitate human-LM interaction in the context of the task. The paper also defines a set of metrics for evaluating human-LM interaction across these tasks.\n\nSources:\n[0] Figure 2: Five tasks and human-LM interaction in the context of accomplishing the tasks.","system":"gpt_retrieval"},{"id":8,"question":"What are the five tasks and human-LM interactions shown in Figure 2?","answer":"Figure 2 shows five tasks and human-LM interactions: social dialogue, question answering, crossword puzzles, text summarization, and metaphor generation [1] [2] [3] [4].\n\nSources:\n[1] Figure 2: Five tasks and human-LM interaction in the context of accomplishing the tasks.\n[2] We then design five tasks ranging from goal-oriented to open-ended to capture different forms of interaction.\n[3] Figure (<>)2 shows five tasks we study in this paper: social dialogue, question answering, crossword puzzles, text summarization, and metaphor generation (see Section (<>)3 for a detailed description of each task).\n[4] Figure (<>)2 shows five tasks we study in this paper: social dialogue, question answering, crossword puzzles, text summarization, and metaphor generation (see Section (<>)3 for a detailed description of each task)."},{"id":11,"question":"How was the performance of different text summarization models evaluated?","answer":"The performance of different text summarization models was evaluated based on automatic metrics and third-party evaluation. Third-party evaluators were asked to evaluate the quality of summaries with respect to their consistency, relevancy, and coherence. Original summaries from Text-Babbage were rated as the most consistent and relevant among the four models based on third-party evaluation. Model performance was also evaluated based on user survey responses. However, there is a discrepancy between the metrics commonly used to evaluate summarization quality and what users find helpful in interacting with and improving these models. Text summarization is a well-established research direction in NLP, where the goal is to compress the salient information in the source document into a coherent and fluent summary.\n\nSources:\n[0] This observation reveals a discrepancy between the metrics commonly used to evaluate summarization quality and what users find helpful in interacting with and improving these models.\n[1] Table 6: [Text summarization] Third-party evaluation of model-generated summaries.\n[2] However, from third-party evaluation, original summaries from Text-Babbage were rated as the most consistent and relevant among the four models (Table (<>)6).\n[3] Table 15: [Text summarization] Model performance based on automatic metrics.\n[4] In text summarization, we ask third-party evaluators to evaluate the quality of summaries with respect to their consistency (i.e., all the information in the summary is inferred from the document), relevancy (i.e., the summary includes only important information and no excess information), and coherence (i.e., the summary organizes the relevant information in a well-structured manner).\n[5] Table 16: [Text summarization] Model performance based on user survey responses..","system":"gpt_retrieval"},{"id":13,"question":"How was the performance of different text summarization models evaluated?","answer":"The performance of different text summarization models was evaluated using a new framework called Human-AI Language-based Interaction Evaluation (HALIE) [1] [2] [3] [4] [5] [6]. HALIE expands non-interactive evaluation along three dimensions, capturing the interactive process, the first-person subjective experience, and notions of preference beyond quality [7] [8] [9] [10] [11]. Five tasks were designed to capture different forms of interaction [12] [13]. Four state-of-the-art language models (LMs) were evaluated: three variants of OpenAI’s GPT-3 and AI21’s J1-Jumbo [14] [15]. HALIE defines metrics on interaction traces, which are organized along three dimensions: targets, perspectives, and criteria [16]. Targets include more than just the final output, and cover the entire interaction process, while perspectives are not limited to third parties, but the users who interact with LMs [17]. From the 1015 interaction traces collected, it was observed that better non-interactive performance does not always lead to better human-LM interaction [18]. Users edited summaries generated by TextBabbage the most, and TextDavinci the least [19]. Overall, users perceived TextDavinci to be most helpful and better improves over time compared to the other models [20]. However, third-party evaluators considered sentences written with the same model to be the worst among the four models [21].\n\nSources:\n[1] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[2] We develop an evaluation framework, Human-AI Language-based Interaction Evaluation (HALIE) that puts interaction at the center.\n[3] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[4] We propose a framework, HALIE, that expands on non-interactive evaluation along three dimensions: (i) we capture the full process in addition to the final output (targets); (ii) we capture the first-person subjective experience of users interacting with the LM in addition to the perspective of a third-party (perspectives), and (iii) we consider notions of preference beyond quality (criteria).\n[5] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[6] To evaluate human-LM interaction, HALIE defines metrics on interaction traces, which are organized along the following three dimensions (Figure (<>)1)—targets, perspectives, and criteria: (1) targets include more than just the final output, and cover the entire interaction process (e.g., user queries and edits); (2) perspectives are not limited to third parties, but the users who interact with LMs\n[7] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[8] We develop an evaluation framework, Human-AI Language-based Interaction Evaluation (HALIE) that puts interaction at the center.\n[9] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[10] We propose a framework, HALIE, that expands on non-interactive evaluation along three dimensions: (i) we capture the full process in addition to the final output (targets); (ii) we capture the first-person subjective experience of users interacting with the LM in addition to the perspective of a third-party (perspectives), and (iii) we consider notions of preference beyond quality (criteria).\n[11] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality.\n[12] We then design five tasks ranging from goal-oriented to open-ended to capture different forms of interaction.\n[13] Figure 2: Five tasks and human-LM interaction in the context of accomplishing the tasks.\n[14] On four state-of-the-art LMs (three variants of OpenAI’s GPT-3 and AI21’s J1-Jumbo), we find that non-interactive performance does not always result in better human-LM interaction and that first-person and third-party metrics can diverge, suggesting the importance of examining the nuances of human-LM interaction.\n[15] With these systems, We evaluated four state-of-the-art LMs: three variants of OpenAI’s GPT-3 ((<>)Brown et al., (<>)2020; (<>)Ouyang et al., (<>)2022)—TextDavinci (text-davinci-001), TextBabbage (text-babbage-001), and Davinci (davinci)— and AI21’s Jurassic-1 ((<>)Lieber et al., (<>)2021)—Jumbo (j1-jumbo).\n[16] To evaluate human-LM interaction, HALIE defines metrics on interaction traces, which are organized along the following three dimensions (Figure (<>)1)—targets, perspectives, and criteria: (1) targets include more than just the final output, and cover the entire interaction process (e.g., user queries and edits); (2) perspectives are not limited to third parties, but the users who interact with LMs\n[17] To evaluate human-LM interaction, HALIE defines metrics on interaction traces, which are organized along the following three dimensions (Figure (<>)1)—targets, perspectives, and criteria: (1) targets include more than just the final output, and cover the entire interaction process (e.g., user queries and edits); (2) perspectives are not limited to third parties, but the users who interact with LMs\n[18] From the 1015 interaction traces we collected, we observe that better non-interactive performance does not always lead to better human-LM interaction (e.g., Section (<>)3.2).\n[19] Table 5: [Text summarization] Users edited summaries generated by TextBabbage the most, and TextDavinci the least.\n[20] Overall, users perceived TextDavinci to be most helpful (helpfulness) and better improves over time (improvement) compared to the other models.\n[21] However, third-party evaluators considered sentences written with the same model to be the worst among the four models (Table (<>)9)."},{"id":16,"question":"What is the purpose of the survey question about changing interaction with the AI Teammate over the course of solving the crossword puzzle?","answer":"The purpose of the survey question about changing interaction with the AI Teammate over the course of solving the crossword puzzle is to closely study engagement in the relatively long period of interaction between users and the AI Teammate in the crossword puzzle task. The survey question is used to measure engagement in two ways and to understand how users chose to interact with the AI Teammate over time. Additionally, the survey question provides insight into how users found the challenge of figuring out how to best use the AI Teammate entertaining. The survey question was included in a survey that each user completed about their experience interacting with the AI Teammate after completing the crossword puzzle. The users were provided with a description of how to interact with the AI Teammate and overall crossword puzzle interface, as well as examples of prompts they could provide to assist with solving puzzle clues. The cumulative queries to the AI Teammate over time for all users were also analyzed to understand how users interacted with the AI Teammate.\n\nSources:\n[0] Example responses to the survey question “Did the way you chose to interact with the AI Teammate change over the course of solving the crossword puzzle?\n[1] For QA and crossword puzzles, we asked users to answer the survey question “Did the way you chose to interact with the AI Teammate change over time?\n[2] The relatively long period of interaction between users and the AI Teammate in the crossword puzzle task provides an opportunity to closely study engagement, which we measure in two ways.\n[3] Change (free-form): Did the way you chose to interact with the AI Teammate change over the course of solving the crossword puzzle?\n[4] Users were provided a description of how to interact with the AI Teammate and overall crossword puzzle interface, as well as examples of prompts they could provide to assist with solving puzzle clues.\n[5] In our setting, users try to complete an entire crossword puzzle with the ability to query an LM-based “AI Teammate” via a chat-based interface.\n[6] Finally, unlike with the use of AI solvers that seek to solve a crossword puzzle perfectly ((<>)Wallace et al., (<>)2022a), some users found the challenge of figuring out how to best use the AI Teammate itself entertaining:\n[7] Afterwards, each user completed a survey about their experience interacting with the AI Teammate.","system":"gpt_retrieval"},{"id":18,"question":"What is the purpose of the survey question about changing interaction with the AI Teammate over the course of solving the crossword puzzle?","answer":"The purpose of the survey question about changing interaction with the AI Teammate over the course of solving the crossword puzzle is to examine user accommodation, or the change in behavior in response to the AI system, both qualitatively and quantitatively [1] [2]. The survey question is used to gather qualitative data on how users adapt their behavior as they learn more about the strengths and weaknesses of the system and the underlying model [3]. The change in query strategies over time is measured quantitatively to analyze how the distribution of prompt type changed for users over time [4] [5]. This analysis helps to understand how these interactions come to affect human language practices, culture, well-being, and broader society [6].\n\nSources:\n[1] For QA and crossword puzzles, we asked users to answer the survey question “Did the way you chose to interact with the AI Teammate change over time?\n[2] We examine user accommodation, or change in behavior in response to the AI system, in two ways: qualitatively, through a free-response survey question, and quantitatively, via measuring the change in query strategies over time.\n[3] One important aspect of human-AI interaction to account for is user accommodation, or the ability of users to adapt their behavior as they learn more about the strengths and weaknesses of a system and the underlying model.\n[4] We examine user accommodation, or change in behavior in response to the AI system, in two ways: qualitatively, through a free-response survey question, and quantitatively, via measuring the change in query strategies over time.\n[5] To further examine these trends quantitatively, we categorized all user prompts using the below taxonomy in order to measure how the distribution of prompt type changed for users over time:\n[6] Overall, especially given the rapid deployment of LMs, including a variety of different interfaces, we recommend that future work should actively monitor how these interactions come to affect human language practices (e.g., writing, reading, listening, speaking), culture, well-being, and broader society."},{"id":20,"question":"what metrics were used for the text summarization task?","answer":"The sources mention several metrics used for evaluating text summarization quality, including fluency, helpfulness, ease, and enjoyment. However, the specific metrics used for the text summarization task are not explicitly stated. Source 0 provides information on model performance based on automatic metrics, while source 7 discusses metrics in general. Source 8 mentions that text summarization is evaluated based on coherence and fluency. Source 9 uses linear regression analysis to calculate means and significance for various variables related to the summarization task. Therefore, it is unclear which specific metrics were used for the text summarization task.\n\nSources:\n[0] Table 15: [Text summarization] Model performance based on automatic metrics.\n[1] The metrics analyzed for the crossword puzzle task included fluency, helpfulness, ease, and enjoyment.\n[2] Metrics.\n[3] Text summarization is a well established research direction in NLP ((<>)Mani, (<>)1999; (<>)Spärck Jones, (<>)1999; (<>)Nenkova and McKeown, (<>)2012), where the goal is to compress the salient information in the source document into a coherent and fluent summary ((<>)Peyrard, (<>)2019)."},{"id":21,"question":"what metrics were used for the text summarization task?","answer":"For the text summarization task, a set of metrics covering the space defined by the three dimensions were defined [1]. Table 1 shows the mapping from the space to the metrics in the five tasks [2] [3]. The task involved users editing a model-generated summary to be consistent, relevant, and coherent, and users were asked to evaluate the consistency, relevance, and coherence of the original (model-generated) summary and edited summary before and after editing [4] [5]. Session-level questions evaluated users’ overall perceptions of the summarization system with respect to its helpfulness and improvement over time [6]. The first three metrics refer to the length of model-generated summaries (original), human-edited summaries (edited), and the Levenshtein distance between them (edit distance) [7]. Other metrics used include consistency, relevance, coherence, density, revision score, and improvement [8] [9] [10] [11] [12] [13].\n\nSources:\n[1] For each task, we define a set of metrics that cover the space defined by the three dimensions.\n[2] Table (<>)1 shows the mapping from the space to the metrics in the five tasks (see Table (<>)21 in Appendix (<>)D for the full list of metrics).\n[3] Table (<>)1 shows the mapping from the space to the metrics in the five tasks (see Table (<>)21 in Appendix (<>)D for the full list of metrics).\n[4] We consider the task, where given a document and a model-generated summary, users edit the summary to be consistent, relevant, and coherent.\n[5] To encourage users to pay attention to the three quality criteria when editing, we asked users to evaluate the consistency, relevance, and coherence of the original (model-generated) summary and edited summary before and after editing.\n[6] Session-level questions evaluated users’ overall perceptions of the summarization system with respect to its helpfulness and improvement over time.\n[7] The first three metrics refer to the length of model-generated summaries (original), human-edited summaries (edited), and the Levenshtein distance between them (edit distance).\n[8] To encourage users to pay attention to the three quality criteria when editing, we asked users to evaluate the consistency, relevance, and coherence of the original (model-generated) summary and edited summary before and after editing.\n[9] According to third-party evaluation, TextBabbage performed the best with respect to consistency and relevance, while TextDavinci was rated highest for coherence.\n[10] According to the density metric (i.e., the average length of the extractive fragment to which each word in the summary belongs) ((<>)Grusky et al., (<>)2018), summaries generated by TextBabbage are much more extractive (16.11) than those by other models (4.19 for TextDavinci, 4.86 for Davinci, and 4.09 for Jumbo).\n[11] Table (<>)5 shows that users found summaries generated by TextDavinci to be the most helpful starting point for editing, receiving the smallest edit distance and revision score along with the highest helpfulness score.\n[12] The survey responses for the perceived amount of revision (revision) accurately reflect the actual edit distance.\n[13] Overall, users perceived TextDavinci to be most helpful (helpfulness) and better improves over time (improvement) compared to the other models."},{"id":22,"question":"what is the impact of instruction tuning on users' performance?","answer":"Instruction tuning can improve the accuracy and efficiency of human-LM interaction, and instruction-tuned models were perceived most favorably in first-party survey evaluation. However, instruction tuning may decrease the specificity of model completions, as shown in the significant drop in performance of TextDavinci compared to Davinci. It is also noted that the helpfulness perceived by users could exceed actual performance improvements. Therefore, while instruction tuning can improve user satisfaction, it may not necessarily correlate with the quality of outputs. It is important to carefully evaluate the impact of instruction tuning on user performance, taking into account factors such as model size, implementation details, and potential confounding effects.\n\nSources:\n[0] Instruction tuning improves accuracy and efficiency for human-LM interaction.\n[1] Instruction tuned models were also perceived most favorably in first-party survey evaluation.\n[2] However, instruction tuning decreased the specificity of model completions as shown in the significant drop in performance of TextDavinci compared to Davinci (Table (<>)2).\n[3] (e.g., Section (<>)3.1), or helpfulness perceived by users could exceed actual performance improvements (e.g., Section (<>)3.3)."},{"id":23,"question":"what is the impact of instruction tuning on users' performance?","answer":"Instruction tuning improves accuracy and efficiency for human-LM interaction [1]. Specifically, Text-Davinci was the most efficient and accurate tool [2]. Users assisted by TextDavinci found their model more fluent, helpful, and easy and enjoyable to interact with compared to other models, and in general provided more accurate solutions across all puzzles [3]. Despite its smaller size, TextBabbage performed better than Davinci on most metrics, demonstrating the effectiveness of instruction tuning for this task [4]. However, for the NYT-1 puzzle, users assisted by TextDavinci were significantly less likely to provide accurate solutions, despite being more likely to find the model helpful [5].\n\nSources:\n[1] Instruction tuning improves accuracy and efficiency for human-LM interaction.\n[2] Table (<>)13 shows that TextDavinci was the most efficient and accurate tool (1.78 queries/question with 69% accuracy).\n[3] Table 4: [Crossword puzzles] Users assisted by TextDavinci found their model more fluent, helpful, and easy and enjoyable to interact with compared to other models, and in general provided more accurate solutions across all puzzles.\n[4] Despite its smaller size, TextBabbage performed better than Davinci on most metrics, demonstrating the effectiveness of instruction tuning for this task.\n[5] In fact, for the NYT-1 puzzle, users assisted by TextDavinci were significantly less likely to provide accurate solutions, despite being more likely to find the model helpful."},{"id":24,"question":"is there a discrepancy between metrics?","answer":"Yes, there is a discrepancy between metrics commonly used to evaluate summarization quality and what users find helpful in interacting with and improving these models. Additionally, some metrics reflect both preference and quality when human preference is aligned with the quality metrics of interest. However, none of the metrics were found to be significantly different from each other, except for the reference model. Metrics can be a function of interaction traces or only depend on outputs. Some metrics are defined based on survey responses from users and third-party evaluators. The metrics analyzed for question answering are elapsed time, number of queries, and user correctness.\n\nSources:\n[0] Some metrics are a function of interaction traces (e.g., number of queries users made), whereas some metrics only depend on outputs (e.g., accuracy in solving a crossword puzzle).\n[1] Also, sometimes metrics can reflect both preference and quality when human preference is aligned with the quality metrics of interest.\n[2] This observation reveals a discrepancy between the metrics commonly used to evaluate summarization quality and what users find helpful in interacting with and improving these models.\n[3] Some metrics are defined based on survey responses from users (e.g., satisfaction over the final output) and third-party evaluators (e.g., consistency of a summary given a document).\n[4] The first three metrics refer to the length of model-generated summaries (original), human-edited summaries (edited), and the Levenshtein distance between them (edit distance).\n[5] The metrics analyzed for question answering are elapsed time, number of queries, and user correctness (accuracy)."},{"id":25,"question":"is there a discrepancy between metrics?","answer":"Yes, there is a discrepancy between metrics used to evaluate language models (LMs) and what users find helpful in interacting with and improving these models [1]. Specifically, non-interactive performance does not always result in better human-LM interaction, and first-person and third-party metrics can diverge [2]. Additionally, sometimes a subset of quality metrics might overpower other metrics when it comes to user preference [3]. Furthermore, a model that achieves the best performance based on third-party perspectives might be the worst based on first-person perspectives, and vice versa [4]. This suggests the importance of examining the nuances of human-LM interaction and evaluating LMs along multiple dimensions and metrics [5].\n\nSources:\n[1] This observation reveals a discrepancy between the metrics commonly used to evaluate summarization quality and what users find helpful in interacting with and improving these models.\n[2] On four state-of-the-art LMs (three variants of OpenAI’s GPT-3 and AI21’s J1-Jumbo), we find that non-interactive performance does not always result in better human-LM interaction and that first-person and third-party metrics can diverge, suggesting the importance of examining the nuances of human-LM interaction.\n[3] Furthermore, sometimes a subset of quality metrics might overpower other metrics when it comes to user preference\n[4] We also find that a model that achieves the best performance based on third-party perspectives might be the worst based on first-person perspectives (e.g., Section (<>)3.4), and visa versa (e.g., Section (<>)3.5).\n[5] To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality."}]},"pdf_url":"/api/v1/static/MARCH_20/2212_09746.pdf"}