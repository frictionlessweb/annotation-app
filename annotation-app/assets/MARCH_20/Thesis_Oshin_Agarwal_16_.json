{"title":"Thesis_Oshin_Agarwal_16_","questions":{"topicTask":[{"id":0,"text":"Named Entity Recognition"},{"id":1,"text":"Entity Names"},{"id":2,"text":"Context"},{"id":3,"text":"Training Data"},{"id":4,"text":"Domain"}],"questionTask":[{"id":5,"text":"What is the impact of temporal changes on the performance of NER models and other tasks?"},{"id":10,"text":"How do the models perform when evaluated on different sub-domains within a seemingly homogenous domain?"},{"id":15,"text":"What is the approach used to audit the in-domain robustness of named entity recognition models, and what are the results?"}],"statementsTask":[{"id":7,"text":"Temporal changes have a significant impact on the performance of NER models and other tasks. The performance of NER models varies significantly even in the same dataset when it is stratified based on news topics. Research on quantifying how model performance changes with time has been sporadic, and studying temporal change in model performance requires extra care in experimental design to tease out the effects. Pre-trained models for language processing tasks are also affected by temporal changes, and the performance of these models varies significantly. However, the temporal change in vocabulary and facts does not affect all tasks as these changes and information might not be necessary to solve all tasks. Models built on pre-trained representations do not experience temporal deterioration on all tasks. Therefore, the impact of temporal changes on the performance of NER models and other tasks is significant and requires careful consideration in experimental design.\n\nSources:\n[0] In chapter (<>)5, we study the impact of temporal changes on the model performance on NER and other tasks.\n[1] Temporal changes Lastly, we discuss the impact of temporal changes on the performance of pre-trained models on downstream tasks.\n[2] We find that the performance of NER models varies significantly even in this dataset when it is stratified based on news topics.\n[3] We find that the performance of NER models varies significantly even in this dataset when it is stratified based on news topics.\n[4] (<https://doi.org/10.1162/tacl_a_00497>)Temporal effects on pre-trained models for language (<https://doi.org/10.1162/tacl_a_00497>)processing tasks.\n[5] Yet research on quantifying how model performance changes with time has been sporadic.\n[6] The temporal change in vocabulary and facts does not affect all tasks as these changes and information might not be necessary to solve all tasks.","system":"gpt_retrieval"},{"id":9,"text":"Temporal changes can have an impact on the performance of pre-trained models for Named Entity Recognition (NER) and other language processing tasks [1] [2] [3] [4]. The performance of pre-trained models on downstream tasks may deteriorate over time due to changes in language and world-related facts [5]. However, the best performance at a given time can be obtained by retraining the system on more recent data [6]. The impact of temporal changes on model performance can be disentangled into temporal model deterioration and temporal domain adaptation [7]. The performance of NER models can suffer due to entity, domain, and temporal effects [8]. However, temporal adaptation via retraining on new gold-standard data is still beneficial [9].\n\nSources:\n[1] From a practical perspective, since named entity recognition models are often developed for targeted applications, we also examine the robustness of models to challenges encountered in practice.\n[2] Specifically, we study the effect of entities from different countries of origin, the effect of fine-grained topics within a domain often treated as homogeneous, and the effects of temporal changes.\n[3] Temporal changes Lastly, we discuss the impact of temporal changes on the performance of pre-trained models on downstream tasks.\n[4] We evaluate the robustness of models for Named Entity Recognition (and other tasks) to changes over time.\n[5] Performance may degrade as language and world-related facts change.\n[6] The central insight from our work is that the performance of pre-trained models on downstream tasks where answer correctness is time-independent, does not necessarily deteriorate over time but that the best performance at a given time can be obtained by retraining the system on more recent data.\n[7] We present experiments (§(<>)5.4 & §(<>)5.5) designed to study temporal effects on downstream language tasks, disentangling worsening model performance due to temporal changes (temporal model deterioration) and the benefit from retraining systems on temporally more recent data in order to obtain optimal performance (temporal domain adaptation).\n[8] The performance of named entity recognition models suffers even within the same genre due to entity, domain and temporal effects.\n[9] Despite this, temporal adaptation via retraining on new gold-standard data is still beneficial."},{"id":12,"text":"The sources suggest that models can have significant performance differences when evaluated on sub-domains within a seemingly homogenous domain. Depending on the sample of sub-domains in the test set, the model performance can vary significantly, leading to an incorrect characterization of the domain/dataset difficulty. Models trained on each sub-domain can have different performance when tested on the same sub-domain and on other sub-domains. The performance on each test sub-domain is reported when a model is trained on sentences from the same sub-domain and when trained on sentences from a different sub-domain. The goal is to determine if it is easier to recognize entities in some sub-domains. Therefore, the models' performance can vary significantly when evaluated on different sub-domains within a seemingly homogenous domain.\n\nSources:\n[0] Sub-domains We also evaluate the robustness of models on sub-domains within a seemingly homogenous domain.\n[1] These two sub-domains, as shown above, exhibit different properties than the rest of the data and therefore including even a few relevant examples helps the models substantially.\n[2] With models built and evaluated on sub-domains within this dataset, we show that there can be significant performance differences even within a seemingly homoge-nous domain.\n[3] Depending upon the sample of sub-domains in the test set, the model performance can vary significantly even in such a homogeneous domain, leading to an incorrect characterization of the domain/dataset difficulty.\n[4] Next, we report the performance of models trained on each sub-domain when tested on the same sub-domain and on other sub-domains.\n[5] (<>)F1 on each test sub-domain with different models.\n[6] Depending upon the sample of sub-domains in the training set, the model performance can vary significantly even in such a homogeneous domain, leading to a much better or worse resulting model.\n[7] Table 15: F1 on each test sub-domain with different models.\n[8] First, we report the performance on each test sub-domain, when a model is trained on sentences from the same sub-domain and when trained on sentences from a different subdomain.\n[9] The goal is to determine if it is easier to recognize entities in some sub-domains.","system":"gpt_retrieval"},{"id":14,"text":"Models for named entity recognition can exhibit significant performance differences even within a seemingly homogenous domain when evaluated on different sub-domains [1] [2] [3]. The F1 score can vary considerably depending on the training and testing sub-domains [4] [5] [6]. It is important to ensure a diverse set of sentences in the training data to improve performance [7] [8]. Additionally, the performance of models can be impacted by entity, domain, and temporal effects [9].\n\nSources:\n[1] In this chapter, we evaluate the robustness of Named Entity Recognition models across topics within a typically “homogeneous” domain, by collecting a new dataset of news articles stratified by news topics.\n[2] While entities unseen in the training data can be a factor that contributes to performance degradation, we find that structural differences in sentences and entity ambiguity are the main contributors.\n[3] With models built and evaluated on sub-domains within this dataset, we show that there can be significant performance differences even within a seemingly homoge-nous domain.\n[4] The results are shown in Table (<>)12.\n[5] Table 12: F1 on each test sub-domain, one per row, with models trained on different domains.\n[6] For OOD, we also report the minimum and maximum F1 obtained by each model along with the corresponding test sub-domain, showing that the range of F1 also varies significantly.\n[7] It is therefore essential to ensure a diverse set of sentences in the training data.\n[8] Even a small number of sentences of each sub-domain in the training data can make a vast difference.\n[9] The performance of named entity recognition models suffers even within the same genre due to entity, domain and temporal effects."},{"id":17,"text":"The approach used to audit the in-domain robustness of named entity recognition models is entity-switched datasets. This approach involves creating datasets where entities are swapped between documents while keeping the rest of the text the same. The results of this approach show that named entity recognition models are not always robust to changes in the entities' national origin and that they can also struggle with challenges encountered in practice. Additionally, the robustness of models to changes over time is also evaluated. Overall, the entity-switched datasets approach is a useful method for auditing the in-domain robustness of named entity recognition models.\n\nSources:\n[0] ((<>)Agarwal et al., (<>)2021b), and also parts of Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2020, Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[1] This chapter is based on content originally published in: Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2020, Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[2] This chapter is based on content originally in: Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2020, Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[3] Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[4] From a practical perspective, since named entity recognition models are often developed for targeted applications, we also examine the robustness of models to challenges encountered in practice.\n[5] In this chapter, we evaluate the robustness of Named Entity Recognition models across topics within a typically “homogeneous” domain, by collecting a new dataset of news articles stratified by news topics.\n[6] We evaluate the robustness of models for Named Entity Recognition (and other tasks) to changes over time.\n[7] We propose a method for auditing the in-domain robustness of systems, focusing on differences in performance due to the national origin of entities.","system":"gpt_retrieval"},{"id":19,"text":"The approach used to audit the in-domain robustness of named entity recognition models is creating entity-switched datasets, in which named entities in the original text are replaced by plausible named entities of the same type while retaining the rest of the text and maintaining its coherence [1] [2]. The entity-switched datasets have a diverse set of entities and the same entity in a variety of contexts by replacing all entities of a given type with the same entity throughout the dataset [3]. State-of-the-art systems are evaluated on these entity-switched datasets, and it is found that they have the highest performance (F1) on American and Indian entities and the lowest performance on Vietnamese and Indonesian entities [4]. A method for auditing the in-domain robustness of systems is proposed, focusing on differences in performance due to the national origin of entities [5]. The models achieve higher F1 on common American names, Russian, Indian, and Mexican names than the original dataset, but performance drops by up to ∼10 points F1 for certain countries [6] [7]. The lower performance can be attributed to both the coverage in the training data and greater ambiguity in names from certain countries of origin [8].\n\nSources:\n[1] This chapter is based on content originally published in: Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2020, Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[2] We create entity-switched datasets ((<>)Agarwal et al., (<>)2020), in which named entities in the original text are replaced by plausible named entities of the same type while retaining the rest of the text and maintaining its coherence.\n[3] Entity-switched datasets not only have a diverse set of entities but also the same entity in a variety of contexts by replacing all entities of a given type with the same entity throughout the dataset.\n[4] We evaluate state-of-the-art systems on these entity-switched datasets and find that they have the highest performance (F1) on American and Indian entities and the lowest performance on Vietnamese and Indonesian entities.\n[5] We propose a method for auditing the in-domain robustness of systems, focusing on differences in performance due to the national origin of entities.\n[6] All the models achieve higher F1 on common American names, Russian, Indian and Mexican names than the original dataset (Super recall).\n[7] For the GloVe models, performance drops by up to ∼10 points F1 for certain countries (Poor recall).\n[8] The lower performance can be attributed to both the coverage in the training data and greater ambiguity in names from certain countries of origin."}],"qaTask":[{"id":6,"question":"What is the impact of temporal changes on the performance of NER models and other tasks?","answer":"Temporal changes have a significant impact on the performance of NER models and other tasks. The performance of NER models varies significantly even in the same dataset when it is stratified based on news topics. Research on quantifying how model performance changes with time has been sporadic, and studying temporal change in model performance requires extra care in experimental design to tease out the effects. Pre-trained models for language processing tasks are also affected by temporal changes, and the performance of these models varies significantly. However, the temporal change in vocabulary and facts does not affect all tasks as these changes and information might not be necessary to solve all tasks. Models built on pre-trained representations do not experience temporal deterioration on all tasks. Therefore, the impact of temporal changes on the performance of NER models and other tasks is significant and requires careful consideration in experimental design.\n\nSources:\n[0] In chapter (<>)5, we study the impact of temporal changes on the model performance on NER and other tasks.\n[1] Temporal changes Lastly, we discuss the impact of temporal changes on the performance of pre-trained models on downstream tasks.\n[2] We find that the performance of NER models varies significantly even in this dataset when it is stratified based on news topics.\n[3] We find that the performance of NER models varies significantly even in this dataset when it is stratified based on news topics.\n[4] (<https://doi.org/10.1162/tacl_a_00497>)Temporal effects on pre-trained models for language (<https://doi.org/10.1162/tacl_a_00497>)processing tasks.\n[5] Yet research on quantifying how model performance changes with time has been sporadic.\n[6] The temporal change in vocabulary and facts does not affect all tasks as these changes and information might not be necessary to solve all tasks.","system":"gpt_retrieval"},{"id":8,"question":"What is the impact of temporal changes on the performance of NER models and other tasks?","answer":"Temporal changes can have an impact on the performance of pre-trained models for Named Entity Recognition (NER) and other language processing tasks [1] [2] [3] [4]. The performance of pre-trained models on downstream tasks may deteriorate over time due to changes in language and world-related facts [5]. However, the best performance at a given time can be obtained by retraining the system on more recent data [6]. The impact of temporal changes on model performance can be disentangled into temporal model deterioration and temporal domain adaptation [7]. The performance of NER models can suffer due to entity, domain, and temporal effects [8]. However, temporal adaptation via retraining on new gold-standard data is still beneficial [9].\n\nSources:\n[1] From a practical perspective, since named entity recognition models are often developed for targeted applications, we also examine the robustness of models to challenges encountered in practice.\n[2] Specifically, we study the effect of entities from different countries of origin, the effect of fine-grained topics within a domain often treated as homogeneous, and the effects of temporal changes.\n[3] Temporal changes Lastly, we discuss the impact of temporal changes on the performance of pre-trained models on downstream tasks.\n[4] We evaluate the robustness of models for Named Entity Recognition (and other tasks) to changes over time.\n[5] Performance may degrade as language and world-related facts change.\n[6] The central insight from our work is that the performance of pre-trained models on downstream tasks where answer correctness is time-independent, does not necessarily deteriorate over time but that the best performance at a given time can be obtained by retraining the system on more recent data.\n[7] We present experiments (§(<>)5.4 & §(<>)5.5) designed to study temporal effects on downstream language tasks, disentangling worsening model performance due to temporal changes (temporal model deterioration) and the benefit from retraining systems on temporally more recent data in order to obtain optimal performance (temporal domain adaptation).\n[8] The performance of named entity recognition models suffers even within the same genre due to entity, domain and temporal effects.\n[9] Despite this, temporal adaptation via retraining on new gold-standard data is still beneficial."},{"id":11,"question":"How do the models perform when evaluated on different sub-domains within a seemingly homogenous domain?","answer":"The sources suggest that models can have significant performance differences when evaluated on sub-domains within a seemingly homogenous domain. Depending on the sample of sub-domains in the test set, the model performance can vary significantly, leading to an incorrect characterization of the domain/dataset difficulty. Models trained on each sub-domain can have different performance when tested on the same sub-domain and on other sub-domains. The performance on each test sub-domain is reported when a model is trained on sentences from the same sub-domain and when trained on sentences from a different sub-domain. The goal is to determine if it is easier to recognize entities in some sub-domains. Therefore, the models' performance can vary significantly when evaluated on different sub-domains within a seemingly homogenous domain.\n\nSources:\n[0] Sub-domains We also evaluate the robustness of models on sub-domains within a seemingly homogenous domain.\n[1] These two sub-domains, as shown above, exhibit different properties than the rest of the data and therefore including even a few relevant examples helps the models substantially.\n[2] With models built and evaluated on sub-domains within this dataset, we show that there can be significant performance differences even within a seemingly homoge-nous domain.\n[3] Depending upon the sample of sub-domains in the test set, the model performance can vary significantly even in such a homogeneous domain, leading to an incorrect characterization of the domain/dataset difficulty.\n[4] Next, we report the performance of models trained on each sub-domain when tested on the same sub-domain and on other sub-domains.\n[5] (<>)F1 on each test sub-domain with different models.\n[6] Depending upon the sample of sub-domains in the training set, the model performance can vary significantly even in such a homogeneous domain, leading to a much better or worse resulting model.\n[7] Table 15: F1 on each test sub-domain with different models.\n[8] First, we report the performance on each test sub-domain, when a model is trained on sentences from the same sub-domain and when trained on sentences from a different subdomain.\n[9] The goal is to determine if it is easier to recognize entities in some sub-domains.","system":"gpt_retrieval"},{"id":13,"question":"How do the models perform when evaluated on different sub-domains within a seemingly homogenous domain?","answer":"Models for named entity recognition can exhibit significant performance differences even within a seemingly homogenous domain when evaluated on different sub-domains [1] [2] [3]. The F1 score can vary considerably depending on the training and testing sub-domains [4] [5] [6]. It is important to ensure a diverse set of sentences in the training data to improve performance [7] [8]. Additionally, the performance of models can be impacted by entity, domain, and temporal effects [9].\n\nSources:\n[1] In this chapter, we evaluate the robustness of Named Entity Recognition models across topics within a typically “homogeneous” domain, by collecting a new dataset of news articles stratified by news topics.\n[2] While entities unseen in the training data can be a factor that contributes to performance degradation, we find that structural differences in sentences and entity ambiguity are the main contributors.\n[3] With models built and evaluated on sub-domains within this dataset, we show that there can be significant performance differences even within a seemingly homoge-nous domain.\n[4] The results are shown in Table (<>)12.\n[5] Table 12: F1 on each test sub-domain, one per row, with models trained on different domains.\n[6] For OOD, we also report the minimum and maximum F1 obtained by each model along with the corresponding test sub-domain, showing that the range of F1 also varies significantly.\n[7] It is therefore essential to ensure a diverse set of sentences in the training data.\n[8] Even a small number of sentences of each sub-domain in the training data can make a vast difference.\n[9] The performance of named entity recognition models suffers even within the same genre due to entity, domain and temporal effects."},{"id":16,"question":"What is the approach used to audit the in-domain robustness of named entity recognition models, and what are the results?","answer":"The approach used to audit the in-domain robustness of named entity recognition models is entity-switched datasets. This approach involves creating datasets where entities are swapped between documents while keeping the rest of the text the same. The results of this approach show that named entity recognition models are not always robust to changes in the entities' national origin and that they can also struggle with challenges encountered in practice. Additionally, the robustness of models to changes over time is also evaluated. Overall, the entity-switched datasets approach is a useful method for auditing the in-domain robustness of named entity recognition models.\n\nSources:\n[0] ((<>)Agarwal et al., (<>)2021b), and also parts of Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2020, Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[1] This chapter is based on content originally published in: Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2020, Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[2] This chapter is based on content originally in: Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2020, Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[3] Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[4] From a practical perspective, since named entity recognition models are often developed for targeted applications, we also examine the robustness of models to challenges encountered in practice.\n[5] In this chapter, we evaluate the robustness of Named Entity Recognition models across topics within a typically “homogeneous” domain, by collecting a new dataset of news articles stratified by news topics.\n[6] We evaluate the robustness of models for Named Entity Recognition (and other tasks) to changes over time.\n[7] We propose a method for auditing the in-domain robustness of systems, focusing on differences in performance due to the national origin of entities.","system":"gpt_retrieval"},{"id":18,"question":"What is the approach used to audit the in-domain robustness of named entity recognition models, and what are the results?","answer":"The approach used to audit the in-domain robustness of named entity recognition models is creating entity-switched datasets, in which named entities in the original text are replaced by plausible named entities of the same type while retaining the rest of the text and maintaining its coherence [1] [2]. The entity-switched datasets have a diverse set of entities and the same entity in a variety of contexts by replacing all entities of a given type with the same entity throughout the dataset [3]. State-of-the-art systems are evaluated on these entity-switched datasets, and it is found that they have the highest performance (F1) on American and Indian entities and the lowest performance on Vietnamese and Indonesian entities [4]. A method for auditing the in-domain robustness of systems is proposed, focusing on differences in performance due to the national origin of entities [5]. The models achieve higher F1 on common American names, Russian, Indian, and Mexican names than the original dataset, but performance drops by up to ∼10 points F1 for certain countries [6] [7]. The lower performance can be attributed to both the coverage in the training data and greater ambiguity in names from certain countries of origin [8].\n\nSources:\n[1] This chapter is based on content originally published in: Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2020, Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[2] We create entity-switched datasets ((<>)Agarwal et al., (<>)2020), in which named entities in the original text are replaced by plausible named entities of the same type while retaining the rest of the text and maintaining its coherence.\n[3] Entity-switched datasets not only have a diverse set of entities but also the same entity in a variety of contexts by replacing all entities of a given type with the same entity throughout the dataset.\n[4] We evaluate state-of-the-art systems on these entity-switched datasets and find that they have the highest performance (F1) on American and Indian entities and the lowest performance on Vietnamese and Indonesian entities.\n[5] We propose a method for auditing the in-domain robustness of systems, focusing on differences in performance due to the national origin of entities.\n[6] All the models achieve higher F1 on common American names, Russian, Indian and Mexican names than the original dataset (Super recall).\n[7] For the GloVe models, performance drops by up to ∼10 points F1 for certain countries (Poor recall).\n[8] The lower performance can be attributed to both the coverage in the training data and greater ambiguity in names from certain countries of origin."},{"id":20,"question":"Who were the committee members?","answer":"The committee members were the COLING 2012 Organizing Committee, the COLING 2016 Organizing Committee, the Dissertation Committee, and the Graduate Group Chairperson. Additionally, there were several individuals listed as authors in sources 6, 7, 8, and 9, but it is unclear if they were committee members.\n\nSources:\n[0] The COLING 2012 Organizing Committee.\n[1] The COLING 2016 Organizing Committee.\n[2] The COLING 2016 Organizing Committee.\n[3] The COLING 2016 Organizing Committee.\n[4] Dissertation Committee"},{"id":21,"question":"Who were the committee members?","answer":"The committee members were Dan Roth, Mark Liberman, Mark Steedman, and Dan Bikel [1].\n\nSources:\n[1] I also thank my thesis committee, Dan Roth, Mark Liberman, Mark Steedman and Dan Bikel, for their insightful comments and questions."},{"id":22,"question":"What are the most interesting findings?","answer":"It is unclear what the most interesting findings are as no specific study or research is mentioned in the sources. However, the sources discuss factors that can affect study findings, textual clues that lead to predictions, cumbersome tables that give a realistic impression of trends, and initial experiments in a certain direction. Additionally, there is mention of a shared task on novel and emerging entity recognition and examples of human evaluation. It is possible that the most interesting findings could be related to any of these topics, but without more information, it is impossible to determine which findings are the most interesting.\n\nSources:\n[0] In Findings\n[1] We highlight the factors that can affect the findings of the study considerably and others that are less important.\n[2] 3we explore this empirically in this thesis\n[3] Here, we examine the textual clues that lead systems to make predictions.\n[4] The resulting tables are cumbersome to analyze but give a realistic impression of the trends.\n[5] experiments.\n[6] Lastly, we present some of our own work with initial experiments in this direction.\n[7] Such analysis can be pursued as future work.\n[8] (<https://doi.org/10.18653/v1/W17-4418>)Results of (<https://doi.org/10.18653/v1/W17-4418>)the WNUT2017 shared task on novel and emerging entity recognition."},{"id":23,"question":"What are the most interesting findings?","answer":"The most interesting findings of the sources are that NER models heavily rely on entity memorization and little on context, which poses a challenge in NER generalization, even within the same domain [1] [2]. Models perform extremely well on entities from certain countries, but not as well on others, which has fairness implications when NER is used in practice [3] [4]. The performance of NER models varies significantly even within the same dataset when it is stratified based on news topics [5] [6]. Additionally, models built on pre-trained representations do not experience temporal deterioration on all tasks, but neural models fine-tuned on the same data but initialized with random vectors for word representation exhibit dramatic temporal deterioration on the same datasets [7].\n\nSources:\n[1] This naturally leads to the models memorizing names which are strong indicators and poses a challenge in NER generalization, even within the same domain.\n[2] We now discuss prior and contemporaneous related work that identified this memorization phenomenon, as well as the methods that incorporate clues based on either the word or the context explicitly as signals in the systems, though without considering their absolute or relative predictiveness.\n[3] We selected entities from different countries and showed that models perform extremely well on entities from certain countries, but not as well on others.\n[4] This finding has fairness implications when NER is used in practice.\n[5] We find that the performance of NER models varies significantly even in this dataset when it is stratified based on news topics.\n[6] In this chapter, we evaluate the robustness of Named Entity Recognition models across topics within a typically “homogeneous” domain, by collecting a new dataset of news articles stratified by news topics.\n[7] We find that models built on pre-trained representations do not experience temporal deterioration on all tasks."},{"id":24,"question":"Is this a strong thesis?","answer":"The sources provide information about the thesis and its development. However, there is no clear indication of whether the thesis is strong or not. The sources mention the goals of the thesis, the challenges it explores, and the contributions of various people to its development. The thesis is a joint effort, and the author acknowledges the support of numerous people. The thesis aims to evaluate the robustness of practical NER in-domain models and determine the feasibility of developing NER models that can identify contexts with strong predictive clues and hence generalize better. The sources also discuss future directions that are relevant to the thesis as a whole and important for broader research. Therefore, based on the available sources, it is not possible to determine whether the thesis is strong or not.\n\nSources:\n[0] This thesis is a joint effort that would not have been possible without the support of numerous people.\n[1] This thesis belongs to him as much as it does to me.\n[2] Unfortunately, the data was not public at the time of writing this thesis and thus, we could not compare our work with it.\n[3] Therefore, the goal of this thesis was two-fold – 1) Evaluate the robustness of practical NER in-domain models, and 2) Determine the feasibility of developing NER models that can identify contexts with strong predictive clues and hence generalize better.\n[4] Here, we discuss future directions that are relevant to the thesis as a whole and important for broader research.\n[5] 3we explore this empirically in this thesis\n[6] They have helped refine the thesis and shape its direction.\n[7] Their suggestions to include figures and definitions have greatly improved the presentation, readability and clarity of the thesis.\n[8] ThesisOutline\n[9] We explore both these challenges in this thesis for the effect of entity, domain and temporal changes on robustness."},{"id":25,"question":"Is this a strong thesis?","answer":"The sources provide information about a thesis on named entity recognition, exploring the role of entity names and context in NER, as well as the generalization ability and robustness of NER models to challenges encountered in practice [1] [2] [3] [4] [5]. The thesis proposes the use of constraining contexts and evaluates the robustness of models on sub-domains and temporal changes [6] [7] [8] [9]. The sources also discuss the use of entity-switched datasets to audit the in-domain robustness of NER models and the interpretability of NER systems [10] [11] [12] [13] [14]. However, there is no information provided on the quality or strength of the thesis itself. Therefore, it is not possible to determine whether this is a strong thesis or not.\n\nSources:\n[1] PRACTICAL NAMED ENTITY RECOGNITION: THE ROLE OF ENTITY AND ITS CONTEXT\n[2] Presented to the Faculties of the University of Pennsylvania\n[3] Degree of Doctor of Philosophy\n[4] In this thesis, we explore the role of entity names and the context (sentence) in which they appear in named entity recognition.\n[5] From a practical perspective, since named entity recognition models are often developed for targeted applications, we also examine the robustness of models to challenges encountered in practice.\n[6] We define a constraining context as the sentence-level context of a word that has strong selectional preferences for the word type.\n[7] We identify and study three different aspects where the performance of such a model may suffer in practice.\n[8] We create entity-switched datasets, replacing named entities in the original text with plausible named entities of the same type but of different national origin.\n[9] In Chapters (<>)3, (<>)4 & (<>)5, we describe the work on entity-switched datasets, performance across sub-domains and temporal effects respectively.\n[10] This chapter is based on content originally published in: Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2020, Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models.\n[11] ((<>)Agarwal et al., (<>)2020), Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2021, Interpretability analysis for named entity recognition to understand system predictions and how they can improve.\n[12] We zeroed in on the question of interpretability of named entity recognition systems, specifically examining the degree of name and context learning using two methods.\n[13] Part of this chapter is based on content originally published in: Oshin Agarwal, Yinfei Yang, Byron C. Wallace and Ani Nenkova, 2021, Interpretability analysis for named entity recognition to understand system predictions and how they can improve.\n[14] ((<>)Agarwal et al., (<>)2021b)."}]},"pdf_url":"/api/v1/static/MARCH_20/Thesis_Oshin_Agarwal_16_.pdf"}